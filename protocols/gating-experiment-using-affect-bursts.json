{"uri":"gating-experiment-using-affect-bursts-rr2d58e","version_id":"0","protocol_name":"Gating Experiment Using Affect Bursts","protocol_name_html":"Gating Experiment Using Affect Bursts","is_retracted":"0","is_prepublished":"0","can_edit":"0","parent_id":null,"api_version":"1","is_new_mode":"0","last_modified":"1541001962","type_id":"1","link":"https:\/\/doi.org\/10.1371\/journal.pone.0206216","fork_id":"","public_fork_note":null,"number_of_steps":"10","has_versions":"0","first_published_date":"1531922253","publish_date":"2018-10-31 16:06:02","documents":null,"have_protocol_in_step":"0","is_protocol_in_step":"0","vendor_name":"Contributed by users","vendor_link":"https:\/\/www.protocols.io","vendor_logo":"\/img\/vendors\/1.png","mod_mins":"-40","mod_secs":"1","description":"<p>The unfolding dynamics of the vocal expression of emotions are crucial for the decoding of the emotional state of an individual. In this study, we analyzed how much information is needed to decode a vocally expressed emotion using affect bursts, a gating paradigm, and linear mixed models. We showed that some emotions (fear, anger, disgust) were significantly better recognized at full-duration than others (joy, sadness, neutral). As predicted, recognition improved when greater proportion of the stimuli were presented. Emotions recognition curves for anger and disgust were best described by higher order polynomials (second to third), while fear, sadness, neutral, and joy were best described by linear relationships. Acoustic features were extracted for each stimulus and subjected to a principal component analysis for each emotion. The principal components were successfully used to partially predict the accuracy of recognition (i.e., for anger, a component encompassing acoustic features such as fundamental frequency (f0) and jitter; for joy, pitch and loudness range).\u00a0 Furthermore, the impact of the principal components on the recognition of anger, disgust, and sadness changed with longer portions being presented. These results support the importance of studying the unfolding conscious recognition of emotional vocalizations to reveal the differential contributions of specific acoustical feature sets over time. It is likely that these effects are due to the relevance of threatening information to the human mind and are related to urgent motor responses when people are exposed to potential threats as compared with emotions where no such urgent response is required (e.g., joy).<\/p>","is_bookmarked":"0","can_reassign":"1","before_start":null,"has_guidelines":"0","materials":[],"materials_text":null,"warning":null,"version_class":"13850","public":"1","is_owner":"1","is_original_owner":"1","created_on":"1531918345","protocol_affiliation":"University of Geneva","affiliation":null,"doi":"dx.doi.org\/10.17504\/protocols.io.rr2d58e","doi_status":"2","changed_fork_steps":null,"profile_url":"w2t234w2v2u2","protocol_img":"https:\/\/s3.amazonaws.com\/pr-journal\/3mk2qsw.png","profile_image":"\/img\/avatars\/012.png","full_name":"Simon Schaerlaeken","created_by":"Simon Schaerlaeken","private_link":"3D70452311040057D3BD55088490B008","original_img":"1","username":"simon-schaerlaeken","retraction_reason":null,"plos_id":"10.1371\/journal.pone.0206216","manuscript_citation":"Schaerlaeken S,  Grandjean D (2018) Unfolding and dynamics of affect bursts decoding in humans. PLoS ONE  13(10): e0206216. doi: <a target=\"_blank\" href=\"https:\/\/dx.doi.org\/10.1371\/journal.pone.0206216\">10.1371\/journal.pone.0206216<\/a> ","journal_name":"PLOS One","journal_link":"https:\/\/doi.org\/10.1371\/journal.pone.0206216","is_donations_disabled":"0","is_donations_disabled_by_user":"9","item_record_id":331911,"fork_info":[],"compare_forks":[],"protocols":[],"groups":[],"number_of_shared_runs":[],"ownership_history":[],"keywords":"gating, affect bursts, emotion, recognition","transfer_to_user":[],"sub_transfer":false,"is_transfer_pending":false,"number_of_bookmarks":"0","collections":[],"tags":[],"archived":0,"sub_authors":[],"sub_protocols_number":0,"can_edit_shared":0,"shared_access":null,"shared_runs":[],"is_shared_run":0,"is_shared":1,"show_comparison":false,"banner":null,"contact_badges":[{"badge_id":"2","badge_image":"\/img\/badges\/bronze.svg","badge_description":"Author"}],"number_of_comments":0,"big_protocol_img":"https:\/\/s3.amazonaws.com\/pr-journal\/3mj2qsw.png","big_protocol_img_ofn":"protocolio.png","is_locked":0,"is_locked_by":false,"authors":"Simon Schaerlaeken,Didier Grandjean","authors_list":[{"name":"Simon Schaerlaeken","affiliation":"University of Geneva","username":"simon-schaerlaeken","profile_image":"\/img\/avatars\/012.png"},{"name":"Didier Grandjean","affiliation":"University of Geneva","username":null,"profile_image":null}],"user":{"profile_image":"\/img\/avatars\/012.png","username":"simon-schaerlaeken","full_name":"Simon Schaerlaeken","created_by":"Simon Schaerlaeken"},"access":{"can_view":"1","can_remove":"0","can_add":"0","can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":"0","can_move":"1","can_transfer":"1","can_download":"1","is_locked":"0"},"is_contact_suspended":0,"guidelines":null,"status_id":"1","is_research":null,"status_info":null,"steps":[{"id":"645392","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"787AC2ABF71D443B9D232DFA37E011C6","previous_guid":null,"data":"{\"blocks\":[{\"key\":\"6ev6d\",\"text\":\"Five emotions were selected for this study: anger, disgust, fear, joy, and sadness. These emotions are thought to show discrete forms of expression in the face and voice. Surprise was excluded from this list because of the difficulty in simulating a realistic expression experimentally.\\u00a0Neutral recordings were also used.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":321,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"de5t\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":[]},{\"key\":\"c2ioh\",\"text\":\"We selected 20\\u00a0recordings from the database for each emotion (except for disgust, which is represented through only 10 recordings because of the poor quality of other recordings).\\u00a0Each recording represented uniquely one of the emotions selected for this study. The emotion expressed in each recording did not change over the course of the recording and was unambiguous.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":369,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"2njib\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]}],\"entityMap\":[{\"type\":\"dataset\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"145\",\"guid\":\"AF74187C55FA43279B0AD1C77D02C590\",\"name\":\"GEneva Multimodal Emotion Portrayals\",\"link\":\"https:\\\/\\\/www.affective-sciences.org\\\/gemep\\\/\",\"can_edit\":true}}]}","section":"Select recordings from the GEneva Multimodal Emotion Portrayals (GEMEP)","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"0","last_modified":"1531918965"},{"id":"645393","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"8165180B0A934132BB669C601E87A3CA","previous_guid":"787AC2ABF71D443B9D232DFA37E011C6","data":"{\"blocks\":[{\"key\":\"2rcid\",\"text\":\"The volume of each recording was normalized to maintain a uniform listening experience\\u00a0between different stimuli (with a coefficient counterbalancing the average and maximum\\u00a0energy) and silences at the beginning and hlthe end of each recording were removed\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":256,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"f791g\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]}],\"entityMap\":[{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"345\",\"guid\":\"E24E3962877A4E9481BC3B62EE274B1C\",\"name\":\"Praat\",\"version\":\"5.3.51\",\"developer\":\"\",\"repository\":\"\",\"link\":\"http:\\\/\\\/www.fon.hum.uva.nl\\\/praat\\\/\",\"os_name\":\"Windows\",\"os_version\":\"\"}}]}","section":"Normalized the recordings and remove silences","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645392","last_modified":"1531919812"},{"id":"645394","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"1B332BE1FD3E4A7C8CF92CA2F4098EBA","previous_guid":"8165180B0A934132BB669C601E87A3CA","data":"{\"blocks\":[{\"key\":\"egu3g\",\"text\":\"Each complete recording were cut into smaller chunks with an incremental duration of 50 ms.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":91,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"7a2cv\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]}],\"entityMap\":[{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"345\",\"guid\":\"3243FF82C1C14202BFCD3603FC10D8C6\",\"name\":\"Praat\",\"version\":\"5.3.51\",\"developer\":\"\",\"repository\":\"\",\"link\":\"http:\\\/\\\/www.fon.hum.uva.nl\\\/praat\\\/\",\"os_name\":\"Windows\",\"os_version\":\"\"}}]}","section":"Create the gates","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645393","last_modified":"1531919874"},{"id":"645395","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"97908F838D6B4FB5841EA94BB90C6F0F","previous_guid":"1B332BE1FD3E4A7C8CF92CA2F4098EBA","data":"{\"blocks\":[{\"key\":\"5dk2p\",\"text\":\"We generated lists of 105 stimuli picked from the whole dataset and presented them in a\\u00a0pseudo-random order.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":108,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"d6j3o\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":[]},{\"key\":\"3okov\",\"text\":\"Three or four different participants evaluated each stimulus.\\u00a0Amongst the 8400 stimuli, we picked 72 to be presented more frequently (10 times) to\\u00a0the participants in order to test the reliability of emotional judgments across the\\u00a0different individuals.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":253,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]}],\"entityMap\":[]}","section":"Generate pseudo-randomized lists of stimuli","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645394","last_modified":null},{"id":"645397","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"D508CC0CF49B4682856DC7838FAD2C64","previous_guid":"97908F838D6B4FB5841EA94BB90C6F0F","data":"{\"blocks\":[{\"key\":\"77rti\",\"text\":\"Participants completed the experiment on a computer. The experiment itself was\\u00a0programmed with Windev, version 15 and ran on\\u00a0computers with a screen resolution of 1,280 x 1,024 pixels. Volume was set to 50% and\\u00a0could be adjusted by the participant. Headphones were provided. The experiment\\u00a0lasted 30 min. The participants had to sign an informed consent form presented in a\\u00a0written form and complete a demographic questionnaire before starting the experiment.\\u00a0The instructions were displayed on the computer screen and a set of three practice trials\\u00a0was presented to the participant (with stimuli not used in the main task). A different\\u00a0list of 105 stimuli out of the entire dataset was presented in a pseudo-random order for\\u00a0each participant. The participant listened to each stimulus and then rated six emotions\\u00a0(anger, fear, joy, sadness, disgust, surprise) and neutral with seven corresponding sliders\\u00a0from 0 to 100. Multiple emotion sliders could be moved but at least one of them had to\\u00a0be moved. The participants were also asked to rate their confidence level for the choice\\u00a0they made. The emotions and confidence levels were evaluated on a continuous scale.\\u00a0This process was repeated for the 105 stimuli used per participant. Finally, the data\\u00a0was anonymised based on the ethics commission's requirements.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":1313,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"79kcs\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]}],\"entityMap\":[{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"346\",\"guid\":\"AFC46E3F81F949EBA0908AA4223C2D44\",\"name\":\"Windev\",\"version\":\"15\",\"developer\":\"PC SOFT\",\"repository\":\"\",\"link\":\"http:\\\/\\\/www.windev.com\",\"os_name\":\"Windows\",\"os_version\":\"\"}}]}","section":"Present stimuli to participants and record responses","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645395","last_modified":"1531920497"},{"id":"645398","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"8D6EACEC4E1A4CF1A9AD9A7874D82BAE","previous_guid":"D508CC0CF49B4682856DC7838FAD2C64","data":"{\"blocks\":[{\"key\":\"82gvc\",\"text\":\"We grouped stimuli into bins representing the percentage of the total\\u00a0duration presented. The different emotions were represented on the same scale\\u00a0independent of their global duration. We created two sets of bins: a general one at 25%\\u00a0intervals (25%-50%-75%-100%) and a more detailed one 10% intervals. For example, if a stimulus, created from an original recording with a duration of 100ms, lasts for 150 ms, it falls into the 25% bin and the 20% bin.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":453,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"7n8ss\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":[]},{\"key\":\"cmo3c\",\"text\":\"NOTE:\\u00a0A bin groups\\u00a0together multiple 'gates,' as defined when we cut the original recordings. However, for\\u00a0the sake of using the same terminology here as in the studies that use a gating\\u00a0paradigm, we also use the term 'gate' to represent the bins in the analysis.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":263,\"style\":\"align-justify\"},{\"offset\":0,\"length\":263,\"style\":\"italic\"}],\"entityRanges\":[],\"data\":[]}],\"entityMap\":[]}","section":"Group stimuli responses by bins","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645397","last_modified":null},{"id":"645415","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"6B8D40C31BF8476BAFAC25921ED8195D","previous_guid":"8D6EACEC4E1A4CF1A9AD9A7874D82BAE","data":"{\"blocks\":[{\"key\":\"6h5fl\",\"text\":\"We decided to compute the unbiased hit rate (Hu) for\\u00a0every emotion at every 10% gate. In order to do so, we first defined that a trial was\\u00a0successfully recognized when the highest value of the continuous emotional scales\\u00a0corresponded to the emotion presented. With this information, we compute a confusion\\u00a0matrix created based on the hits and misses of all the participants for each emotion at\\u00a0every 10% gates. We also computed Hu score, for each participant, based on\\u00a0a personal confusion matrix computed for every emotion at every 25% gate. Only the\\u00a0cruder gates were used for this case to provide enough data for the computation of Hu.\\u00a0Since each participant was associated with his\\\/her own set of Hu score, this provided us\\u00a0with a Hu score variability at every 25% gate allowing to compute statistical models.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":813,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]}],\"entityMap\":[]}","section":"Computation of Hu Scores","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645398","last_modified":null},{"id":"645419","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"DD3B8D0C02664213A8D4D896A1383C5E","previous_guid":"8B25CA277DD9422081BC648FAB1F5FD2","data":"{\"blocks\":[{\"key\":\"t9ff\",\"text\":\"We used these Hu score for each participant to examine the accuracy of the recognition\\u00a0of emotions across all trials per gate and emotion using linear mixed models (LMMs).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":171,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"e0npa\",\"text\":\"We used chi-square difference tests to investigate the contribution of\\u00a0each variable and their interaction. The fixed effects were the emotion expressed (anger,\\u00a0sadness, joy, disgust, fear, neutral) and the duration of the gates (25-50-75-100%). The random intercepts effects encapsulated the variability related to\\u00a0each participant. We used a step-up strategy while building the model to test the\\u00a0different combinations of fixed effects.\\u00a0The LMM estimated recognition curves, consisting of unbiased recognition of a specific emotions computed at every gate.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":558,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"ccd5l\",\"text\":\"The 4 components from the Principal Component Analysis of the acoustic features\\u00a0were also used as fixed effect in LMM to map it to the\\u00a0Hu score.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":144,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"83kue\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]}],\"entityMap\":[{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"347\",\"guid\":\"9EC24DC3DAB14522AAE4BA3A1ED6B31B\",\"name\":\"lme4\",\"version\":\"1.1-17\",\"developer\":\"Douglas Bates, Martin Maechler, Ben Bolker, Steve Walker\",\"repository\":\"\",\"link\":\"https:\\\/\\\/cran.r-project.org\\\/web\\\/packages\\\/lme4\\\/index.html\",\"os_name\":\"Windows\",\"os_version\":\"\"}}]}","section":"Linear mixed models","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645421","last_modified":"1531921398"},{"id":"645420","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"04DFED8E2CA0496FB0924495FF13CF88","previous_guid":"DD3B8D0C02664213A8D4D896A1383C5E","data":"{\"blocks\":[{\"key\":\"1887k\",\"text\":\"We\\u00a0computed a polynomial contrast for each recognition curve to determine the nonlinear\\u00a0shape of the curve over time. For the 10 gates model, we computed polynomial\\u00a0regression and the corresponding root mean square error (RMSE).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[{\"offset\":0,\"length\":228,\"style\":\"align-justify\"}],\"entityRanges\":[],\"data\":[]},{\"key\":\"35u59\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":[]},{\"key\":\"e91mt\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]},{\"key\":\"916l3\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":1}],\"data\":[]}],\"entityMap\":[{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"348\",\"guid\":\"673F6498482D4F1E9ABA05334CCAAC6C\",\"name\":\"Phia\",\"version\":\"0.2-1\",\"developer\":\"Helios De Rosario-Martinez\",\"repository\":\"\",\"link\":\"https:\\\/\\\/CRAN.R-project.org\\\/package=phia\",\"os_name\":\"Windows\",\"os_version\":\"\"}},{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"349\",\"guid\":\"66CA09DF89444BEBA4D3D51DF0A39599\",\"name\":\"Matlab\",\"version\":\"16b\",\"developer\":\"MathWorks\",\"repository\":\"\",\"link\":\"https:\\\/\\\/ch.mathworks.com\\\/fr\\\/products\\\/matlab.html\",\"os_name\":\"Windows\",\"os_version\":\"\"}}]}","section":"Recognition curves fit and polynomial contrasts","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645419","last_modified":"1531921549"},{"id":"645421","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"8B25CA277DD9422081BC648FAB1F5FD2","previous_guid":"6B8D40C31BF8476BAFAC25921ED8195D","data":"{\"blocks\":[{\"key\":\"5auro\",\"text\":\"We estimated\\u00a0a set of 42 acoustical features proposed by the Geneva Minimalist Acoustic Parameter\\u00a0Set, developed for emotional prosody research. Dimensionality reduction was\\u00a0applied through a principal component analysis (PCA) specific to each emotion. We\\u00a0selected four components for each emotion based on the cumulative sum of eigenvalues.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":[]},{\"key\":\"fvmvq\",\"text\":\" \",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":[]},{\"key\":\"aamjs\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":0}],\"data\":[]},{\"key\":\"372no\",\"text\":\" \",\"type\":\"atomic\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":0,\"length\":1,\"key\":1}],\"data\":[]}],\"entityMap\":[{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"350\",\"guid\":\"93C0747D4787495F9DAD08CDAED94BB4\",\"name\":\"openSMILE\",\"version\":\"\",\"developer\":\"audEERING\",\"repository\":\"\",\"link\":\"https:\\\/\\\/audeering.com\\\/technology\\\/opensmile\\\/\",\"os_name\":\"Windows\",\"os_version\":\"\"}},{\"type\":\"software\",\"mutability\":\"MUTABLE\",\"data\":{\"id\":\"349\",\"guid\":\"42324EF0FCC34AF89EAFBB63EC295CF9\",\"name\":\"Matlab\",\"version\":\"16b\",\"developer\":\"MathWorks\",\"repository\":\"\",\"link\":\"https:\\\/\\\/ch.mathworks.com\\\/fr\\\/products\\\/matlab.html\",\"os_name\":\"Windows\",\"os_version\":\"\"}}]}","section":"Computation of acoustical features","case_id":"0","section_color":null,"protocol_id":"13850","previous_id":"645415","last_modified":"1531921894"}]}