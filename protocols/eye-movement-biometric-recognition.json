{"uri":"eye-movement-biometric-recognition-muqc6vw","version_id":"1","protocol_name":"eye movement biometric recognition","protocol_name_html":"eye movement biometric recognition","is_prepublished":"0","can_edit":"0","parent_id":"9812","api_version":"1","is_new_mode":"0","last_modified":"1522944362","type_id":"1","link":"https:\/\/doi.org\/10.1371\/journal.pone.0194475","fork_id":"","public_fork_note":null,"number_of_steps":"4","has_versions":"1","first_published_date":"1516978837","publish_date":"2018-04-05 16:06:02","documents":null,"have_protocol_in_step":"0","is_protocol_in_step":"0","vendor_name":"Contributed by users","vendor_link":"https:\/\/www.protocols.io","vendor_logo":"\/img\/vendors\/1.png","mod_mins":"-45","mod_secs":"1","description":"<p>Protocal for eye movment biometric recognition<\/p>","is_bookmarked":"0","can_reassign":"1","before_start":null,"has_guidelines":"0","materials":[],"warning":null,"version_class":"9812","public":"1","is_owner":"1","is_original_owner":"1","created_on":"1516936633","protocol_affiliation":"Beijing Institute of Radiation Medicine, State Key Laboratory of Proteomics, Cognitive and Mental Health Research Center","affiliation":null,"doi":"dx.doi.org\/10.17504\/protocols.io.muqc6vw","doi_status":"2","changed_fork_steps":null,"profile_url":"y2x2b4z2v2","protocol_img":"https:\/\/www.protocols.io\/img\/default_protocol.png","profile_image":"\/img\/avatars\/001.png","full_name":"Chunyong Li","created_by":"Chunyong Li","private_link":"0ECF7D679A07BC272C2A50B7341D5AD5","original_img":"1","username":"chunyong-li","is_retracted":"0","retraction_reason":null,"plos_id":"10.1371\/journal.pone.0194475","manuscript_citation":"Li C,  Xue J,  Quan C,  Yue J,  Zhang C (2018) Biometric recognition via texture features of eye movement trajectories in a visual searching task. PLoS ONE  13(4): e0194475. doi: <a target=\"_blank\" href=\"https:\/\/dx.doi.org\/10.1371\/journal.pone.0194475\">10.1371\/journal.pone.0194475<\/a> ","journal_name":"PLOS One","is_donations_disabled":"0","is_donations_disabled_by_user":"9","item_record_id":282781,"fork_info":[],"compare_forks":[],"protocols":[],"groups":[],"number_of_shared_runs":[],"ownership_history":[],"keywords":"eye movement, biometric, visual searching task, Gabor wavelets","transfer_to_user":[],"sub_transfer":false,"is_transfer_pending":false,"number_of_bookmarks":"0","collections":[],"tags":[],"archived":0,"sub_authors":[],"sub_protocols_number":0,"can_edit_shared":0,"shared_runs":[],"is_shared_run":0,"is_shared":1,"banner":null,"contact_badges":[],"number_of_comments":0,"is_locked":0,"is_locked_by":false,"authors":"Chunyong Li,Jiguo Xue,Cheng Quan,Jingwei Yue,Chenggang Zhang","authors_list":[{"name":"Chunyong Li","affiliation":"Beijing Institute of Radiation Medicine, State Key Laboratory of Proteomics, Cognitive and Mental Health Research Center","username":null,"profile_image":null},{"name":"Jiguo Xue","affiliation":"Beijing Institute of Radiation Medicine, State Key Laboratory of Proteomics, Cognitive and Mental Health Research Center","username":null,"profile_image":null},{"name":"Cheng Quan","affiliation":"Beijing Institute of Radiation Medicine, State Key Laboratory of Proteomics, Cognitive and Mental Health Research Center","username":null,"profile_image":null},{"name":"Jingwei Yue","affiliation":"Beijing Institute of Radiation Medicine, State Key Laboratory of Proteomics, Cognitive and Mental Health Research Center","username":null,"profile_image":null},{"name":"Chenggang Zhang","affiliation":"Beijing Institute of Radiation Medicine, State Key Laboratory of Proteomics, Cognitive and Mental Health Research Center","username":null,"profile_image":null}],"user":{"profile_image":"\/img\/avatars\/001.png","username":"chunyong-li","full_name":"Chunyong Li","created_by":"Chunyong Li"},"access":{"can_view":"1","can_remove":"0","can_add":"0","can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":"0","can_move":"1","can_transfer":"1","can_download":"1","is_locked":"0"},"is_contact_suspended":0,"guidelines":null,"status_id":"1","is_research":null,"status_info":null,"steps":[{"id":"605061","is_changed":"0","original_id":"604899","is_skipped":"0","is_checked":"0","guid":"DDF6DDC35F2C488CA18CE25B60A44979","previous_guid":null,"previous_id":null,"last_modified":"1516936633","components":[{"component_id":"1048136","previous_id":0,"original_id":"1047729","guid":"A1AB7E62645140F896639BB7415155B6","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>1. Design the visual searching task with HTML, which wil be used in the practice part<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smvishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smvishw.png\" data-ofn=\"1.png\" \/><\/p>\n<p>2. Design the data collection test<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smwishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smwishw.png\" data-ofn=\"2.png\" \/><\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smxishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smxishw.png\" data-ofn=\"3.png\" \/><\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smyishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smyishw.png\" data-ofn=\"4.png\" \/><\/p>\n<p>\u00a0<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>1. Design the visual searching task with HTML, which wil be used in the practice part<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smvishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smvishw.png\" data-ofn=\"1.png\" \/><\/p>\n<p>2. Design the data collection test<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smwishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smwishw.png\" data-ofn=\"2.png\" \/><\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smxishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smxishw.png\" data-ofn=\"3.png\" \/><\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smyishw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/smyishw.png\" data-ofn=\"4.png\" \/><\/p>\n<p>\u00a0<\/p>"},"is_project":0},{"component_id":"1048137","previous_id":"1048136","original_id":"1047730","guid":"6924BD2B20A24175A99832501DD03BFE","previous_guid":"A1AB7E62645140F896639BB7415155B6","component_type_id":"6","data_id":null,"data":"Visual searching task and data collection test design","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Visual searching task and data collection test design"},"is_project":0}]},{"id":"605062","is_changed":"0","original_id":"604900","is_skipped":"0","is_checked":"0","guid":"FEF5A27EDF674102BEDFC1D3C87ABB6E","previous_guid":"DDF6DDC35F2C488CA18CE25B60A44979","previous_id":"605061","last_modified":"1516936633","components":[{"component_id":"1048138","previous_id":0,"original_id":"1047731","guid":"0C0C1E8683684DB880286B6C0CF29E64","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>Eye movement data collection procedure is the same for every participant.<\/p>\n<p>\u00a0<\/p>\n<p>1, Practice visual searching task with the HTML visual searching task (at least 40 questions).<\/p>\n<p>2, Collect eye movement data with trial1\u00a0(pretest1, test1, test2, test3, test4) of the data collection test.<\/p>\n<p>\u00a0<\/p>\n<p>After two weeks<\/p>\n<p>\u00a0<\/p>\n<p>3, Collect eye movement data with trial2\u00a0\u00a0(pretest2, test5, test6, test7, test8)\u00a0of the data collection test.<\/p>\n<p>\u00a0<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>Eye movement data collection procedure is the same for every participant.<\/p>\n<p>\u00a0<\/p>\n<p>1, Practice visual searching task with the HTML visual searching task (at least 40 questions).<\/p>\n<p>2, Collect eye movement data with trial1\u00a0(pretest1, test1, test2, test3, test4) of the data collection test.<\/p>\n<p>\u00a0<\/p>\n<p>After two weeks<\/p>\n<p>\u00a0<\/p>\n<p>3, Collect eye movement data with trial2\u00a0\u00a0(pretest2, test5, test6, test7, test8)\u00a0of the data collection test.<\/p>\n<p>\u00a0<\/p>"},"is_project":0},{"component_id":"1048139","previous_id":"1048138","original_id":"1047732","guid":"F21E3AAEE1D94BD183B2443D2397A4B4","previous_guid":"0C0C1E8683684DB880286B6C0CF29E64","component_type_id":"6","data_id":null,"data":"Eye movement data collection","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Eye movement data collection"},"is_project":0}]},{"id":"605063","is_changed":"0","original_id":"604904","is_skipped":"0","is_checked":"0","guid":"846B50D299AF4C4EA693196C3E02BBAB","previous_guid":"FEF5A27EDF674102BEDFC1D3C87ABB6E","previous_id":"605062","last_modified":"1516936634","components":[{"component_id":"1048140","previous_id":0,"original_id":"1047740","guid":"C6060C1D45AB4398AAE818B668F16828","previous_guid":null,"component_type_id":"6","data_id":null,"data":"Feature extraction","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Feature extraction"},"is_project":0},{"component_id":"1048141","previous_id":"1048140","original_id":"1047739","guid":"F4F37B253688467291E66F5678E3100A","previous_guid":"C6060C1D45AB4398AAE818B668F16828","component_type_id":"1","data_id":null,"data":"<p>1, Draw the eye movement trajectory pictures with raw gaze data.<\/p>\n<p>\u00a0<img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snjishw.jpg\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snjishw.jpg\" data-ofn=\"4.jpg\" \/><\/p>\n<p>2, Get texture map of these eye movement trajectory pictures with Gabor wavelets.<\/p>\n<p>\u00a0<img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snkishw.jpg\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snkishw.jpg\" data-ofn=\"5.jpg\" \/><\/p>\n<p>3. Calculate the mean and varience of these texture maps, these values are the characteristic values.<\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>1, Draw the eye movement trajectory pictures with raw gaze data.<\/p>\n<p>\u00a0<img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snjishw.jpg\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snjishw.jpg\" data-ofn=\"4.jpg\" \/><\/p>\n<p>2, Get texture map of these eye movement trajectory pictures with Gabor wavelets.<\/p>\n<p>\u00a0<img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snkishw.jpg\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/snkishw.jpg\" data-ofn=\"5.jpg\" \/><\/p>\n<p>3. Calculate the mean and varience of these texture maps, these values are the characteristic values.<\/p>"},"is_project":0}]},{"id":"605064","is_changed":"0","original_id":"604907","is_skipped":"0","is_checked":"0","guid":"87C3323E3B1B43E7A59A80B16F5D4B36","previous_guid":"846B50D299AF4C4EA693196C3E02BBAB","previous_id":"605063","last_modified":"1516937252","components":[{"component_id":"1048142","previous_id":0,"original_id":"1047747","guid":"F33A00BE10014A518D7CC7D9FBCB1AFB","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>1, Calculate the\u00a0intrinsic dimension of the 176 texture features based on the \u2018maximum likelihood\u2019 (ML) algorithm.<\/p>\n<p>2, Reduce the demension of textrure features to its intrinsic dimension\u00a0\u00a0with the \u2018linear discriminant analysis\u2019 (LDA) algorithm.<\/p>\n<p>4. Train (n-1)*n\/2 SVM classifiers with training datasets (70% of toltal dataset)\u00a0 (the n is the number of participants).<\/p>\n<p>5. For verification scenario, an unclassified test feature will be classified with n-1 SVM classifiers to determine whether it belongs to a participant or not.<\/p>\n<p>\u00a0 \u00a0 \u00a0For identification scenario, an unclassified test feature will be classified with (n-1)*n\/2 to determine which paiticipant it belongs to.<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>1, Calculate the\u00a0intrinsic dimension of the 176 texture features based on the \u2018maximum likelihood\u2019 (ML) algorithm.<\/p>\n<p>2, Reduce the demension of textrure features to its intrinsic dimension\u00a0\u00a0with the \u2018linear discriminant analysis\u2019 (LDA) algorithm.<\/p>\n<p>4. Train (n-1)*n\/2 SVM classifiers with training datasets (70% of toltal dataset)\u00a0 (the n is the number of participants).<\/p>\n<p>5. For verification scenario, an unclassified test feature will be classified with n-1 SVM classifiers to determine whether it belongs to a participant or not.<\/p>\n<p>\u00a0 \u00a0 \u00a0For identification scenario, an unclassified test feature will be classified with (n-1)*n\/2 to determine which paiticipant it belongs to.<\/p>"},"is_project":0},{"component_id":"1048143","previous_id":"1048142","original_id":"1047748","guid":"1B013024DC3C419E99988DC29B17C6E9","previous_guid":"F33A00BE10014A518D7CC7D9FBCB1AFB","component_type_id":"6","data_id":null,"data":"Feature recognition","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Feature recognition"},"is_project":0}]}]}