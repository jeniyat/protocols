{"uri":"deep-learning-for-plant-phenotyping-jcncive","version_id":"0","protocol_name":"Deep Learning for Plant Phenotyping","protocol_name_html":"Deep Learning for Plant Phenotyping","is_prepublished":"0","can_edit":"0","parent_id":null,"api_version":"1","is_new_mode":"0","last_modified":"1521784801","type_id":"1","link":"https:\/\/doi.org\/10.1093\/gigascience\/gix083","fork_id":"","public_fork_note":"","number_of_steps":"5","has_versions":"0","first_published_date":"1502451115","publish_date":"2017-08-11 11:31:55","documents":null,"have_protocol_in_step":"0","is_protocol_in_step":"0","vendor_name":"Contributed by users","vendor_link":"https:\/\/www.protocols.io","vendor_logo":"\/img\/vendors\/1.png","mod_mins":"-45","mod_secs":"1","description":"<p>Deep learning is an emerging field that promises unparalleled results on many data analysis problems. We show the success offered by such techniques when applied to the challenging problem of image-based plant phenotyping, and demonstrate state-of-the-art results for root and shoot feature identification and localisation. We use fully automated trait identification using deep learning to identify quantitative trait loci in root architecture datasets. The majority of (12 out of 14) manuallyidentified QTL were also discovered using our automated approach based on the deep learning detection to locate plant features. We predict a paradigm shift in image-based phenotyping bought<br \/>about by deep learning approaches.<\/p>","is_bookmarked":"0","can_reassign":"1","before_start":null,"has_guidelines":"0","materials":[],"warning":null,"version_class":"7278","public":"1","is_owner":"1","is_original_owner":"1","created_on":"1502449276","protocol_affiliation":"University of Nottingham","affiliation":null,"doi":"dx.doi.org\/10.17504\/protocols.io.jcncive","doi_status":"1","changed_fork_steps":"","profile_url":"x2235413u2","protocol_img":"https:\/\/s3.amazonaws.com\/pr-journal\/mvchewn.png","profile_image":"\/img\/avatars\/017.png","full_name":"Michael Pound","created_by":"Michael Pound","private_link":"C7952C6934932737DF0AC36389908020","original_img":"1","username":"michael-pound","is_retracted":"0","retraction_reason":null,"plos_id":"10.1093\/gigascience\/gix083","manuscript_citation":"Pound MP,  Atkinson JA,  Townsend AJ,  Wilson MH,  Griffiths M,  Jackson AS,  Bulat A,  Tzimiropoulos G,  Wells DM,  Murchie EH,  Pridmore TP,  French AP, Deep machine learning provides state-of-the-art performance in image-based plant phenotyping. GigaScience  6(10). doi: <a target=\"_blank\" href=\"https:\/\/dx.doi.org\/10.1093\/gigascience\/gix083\">10.1093\/gigascience\/gix083<\/a> ","journal_name":"GigaScience","is_donations_disabled":"0","is_donations_disabled_by_user":"9","item_record_id":259618,"fork_info":[],"compare_forks":[{"protocol_id":"8975","protocol_name":"Ant Genera Identification Using an Ensemble of Convolutional Neural Networks","protocol_name_html":"Ant Genera Identification Using an Ensemble of Convolutional Neural Networks","uri":"ant-genera-identification-using-an-ensemble-of-con-kzpcx5n","full_name":"Marcos M. Raimundo","affiliation":null,"affiliation_url":null,"username":"marcos-m-raimundo"}],"protocols":[],"groups":[{"group_id":"643","group_uri":"plantae","group_name":"Plantae","group_logo":"https:\/\/s3.amazonaws.com\/pr-journal\/urzeiee.jpg","requested_uid":"17424","request_flag":"1","my_request":"1"}],"number_of_shared_runs":[],"ownership_history":[],"keywords":"machine learning, ","transfer_to_user":[],"sub_transfer":false,"is_transfer_pending":false,"number_of_bookmarks":"2","collections":[],"tags":[{"tag_id":"812","tag_name":" plant phenotyping"},{"tag_id":"815","tag_name":" wheat"},{"tag_id":"818","tag_name":"machine learning"},{"tag_id":"819","tag_name":" deep learning"},{"tag_id":"820","tag_name":" roots"},{"tag_id":"821","tag_name":" shoots"},{"tag_id":"822","tag_name":" caffe"},{"tag_id":"823","tag_name":" python"}],"archived":0,"sub_authors":[],"sub_protocols_number":0,"can_edit_shared":0,"shared_runs":[],"is_shared_run":0,"is_shared":1,"banner":null,"contact_badges":[{"badge_id":"2","badge_image":"\/img\/badges\/bronze.svg","badge_description":"Author!"},{"badge_id":"5","badge_image":"\/img\/badges\/earlyadopter.svg","badge_description":"Early adopter"}],"number_of_comments":0,"big_protocol_img":"https:\/\/s3.amazonaws.com\/pr-journal\/mvbhewn.png","big_protocol_img_ofn":"sup2-high-res.png","is_locked":0,"is_locked_by":false,"authors":"Michael Pound,Andrew French","authors_list":[{"name":"Michael Pound","affiliation":"University of Nottingham","username":null,"profile_image":null},{"name":"Andrew French","affiliation":"University of Nottingham","username":null,"profile_image":null}],"user":{"profile_image":"\/img\/avatars\/017.png","username":"michael-pound","full_name":"Michael Pound","created_by":"Michael Pound"},"access":{"can_view":"1","can_remove":"0","can_add":"0","can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":"0","can_move":"1","can_transfer":"1","can_download":"1","is_locked":"0"},"is_contact_suspended":0,"guidelines":null,"status_id":"1","is_research":"1","status_info":null,"steps":[{"id":"582911","is_changed":0,"original_id":"0","is_skipped":"0","is_checked":"0","guid":"9FEB635FE5A149129B9D9A89EA567B90","previous_guid":null,"previous_id":"0","last_modified":"1502451046","components":[{"component_id":"969826","previous_id":0,"original_id":"0","guid":"38BDA66551D043F2BCF90B340B379A61","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>This protocol requires the caffe deep learning library, along with python. Detailed instructions exist on the Caffe website. Once installed, the following should produce no errors (using the root CNN architecture as an example):<\/p>\n<pre class=\"language-markup\"><code>import caffe\nimport lmdb\nimport numpy as np\n\nnet = caffe.Net('root_model.prototxt',caffe.TEST)\ncaffe.set_mode_gpu()<\/code><\/pre>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>This protocol requires the caffe deep learning library, along with python. Detailed instructions exist on the Caffe website. Once installed, the following should produce no errors (using the root CNN architecture as an example):<\/p>\n<pre class=\"language-markup\"><code>import caffe\nimport lmdb\nimport numpy as np\n\nnet = caffe.Net('root_model.prototxt',caffe.TEST)\ncaffe.set_mode_gpu()<\/code><\/pre>"},"is_project":0},{"component_id":"969827","previous_id":"969826","original_id":"0","guid":"060BEC98BCDA4EAC916380DE68B80C24","previous_guid":"38BDA66551D043F2BCF90B340B379A61","component_type_id":"6","data_id":"0","data":"Install Prerequisite Libraries","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Install Prerequisite Libraries"},"is_project":0},{"component_id":"969828","previous_id":"969827","original_id":"0","guid":"8221F7196A0945E1A2BA8F9C7B666A48","previous_guid":"060BEC98BCDA4EAC916380DE68B80C24","component_type_id":"17","data_id":"1154","data":"<p>You should receive no errors when running caffe on the command line, or importing caffe within python.<\/p>","order_id":"2","name":"Expected result","data_by_id":"1","type_id":"17","source_data":{"result":"<p>You should receive no errors when running caffe on the command line, or importing caffe within python.<\/p>"},"is_project":0}],"available_protocols":[]},{"id":"582912","is_changed":0,"original_id":"0","is_skipped":"0","is_checked":"0","guid":"BEE82A1D2DFE4924BE7611119066C56B","previous_guid":"9FEB635FE5A149129B9D9A89EA567B90","previous_id":"582911","last_modified":"1502451019","components":[{"component_id":"969829","previous_id":0,"original_id":"0","guid":"BF54CCA4C8F84D98BD3191EAF7C5C137","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>In order to scan images and perform localisation, a network must first be trained to classify individual samples of the images. In our work these samples were either 32x32 RGB images for roots, or 64x64 RGB images for shoots.<\/p>\n<p>\u00a0<\/p>\n<p>The makedb.py script will combine multiple images in folders into two LMDB database files containing training and testing images. The required input is one folder of images per class. For example with root tip classification there are two classes (negative, positive) with shoot classification there are five (negative, leaf tip, leaf base, ear tip, ear base).<\/p>\n<p>\u00a0<\/p>\n<p>\u00a0<img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu7hewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu7hewn.png\" data-ofn=\"p2.png\" \/><\/p>\n<p>\u00a0<\/p>\n<p>makedb.py can be executed as follows:<\/p>\n<pre class=\"language-markup\"><code>python makedb.py training-ratio output-prefix class1 class2 class3 ...<\/code><\/pre>\n<p>E.g.<\/p>\n<pre class=\"language-markup\"><code>python makedb.py 0.8 .\/data\/db1 .\/data\/negative .\/data\/positive<\/code><\/pre>\n<p>Will create db1_training and db1_testing folders, containing training and testing databses with an 80% ratio of training:testing images.<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu8hewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu8hewn.png\" data-ofn=\"p1.png\" \/><\/p>\n<p>\u00a0<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>In order to scan images and perform localisation, a network must first be trained to classify individual samples of the images. In our work these samples were either 32x32 RGB images for roots, or 64x64 RGB images for shoots.<\/p>\n<p>\u00a0<\/p>\n<p>The makedb.py script will combine multiple images in folders into two LMDB database files containing training and testing images. The required input is one folder of images per class. For example with root tip classification there are two classes (negative, positive) with shoot classification there are five (negative, leaf tip, leaf base, ear tip, ear base).<\/p>\n<p>\u00a0<\/p>\n<p>\u00a0<img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu7hewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu7hewn.png\" data-ofn=\"p2.png\" \/><\/p>\n<p>\u00a0<\/p>\n<p>makedb.py can be executed as follows:<\/p>\n<pre class=\"language-markup\"><code>python makedb.py training-ratio output-prefix class1 class2 class3 ...<\/code><\/pre>\n<p>E.g.<\/p>\n<pre class=\"language-markup\"><code>python makedb.py 0.8 .\/data\/db1 .\/data\/negative .\/data\/positive<\/code><\/pre>\n<p>Will create db1_training and db1_testing folders, containing training and testing databses with an 80% ratio of training:testing images.<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu8hewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu8hewn.png\" data-ofn=\"p1.png\" \/><\/p>\n<p>\u00a0<\/p>"},"is_project":0},{"component_id":"969830","previous_id":"969829","original_id":"0","guid":"7BBA80384E4B4A96880C3B1E18335810","previous_guid":"BF54CCA4C8F84D98BD3191EAF7C5C137","component_type_id":"6","data_id":"0","data":"Prepare a classification dataset","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Prepare a classification dataset"},"is_project":0},{"component_id":"969833","previous_id":"969830","original_id":"0","guid":"C6F00F51A68B4026B6E5875C3C11D123","previous_guid":"7BBA80384E4B4A96880C3B1E18335810","component_type_id":"17","data_id":"1155","data":"<p>Your directory structure should now contain two folders of lmdb files, holding the training and testing images.<\/p>","order_id":"2","name":"Expected result","data_by_id":"1","type_id":"17","source_data":{"result":"<p>Your directory structure should now contain two folders of lmdb files, holding the training and testing images.<\/p>"},"is_project":0}],"available_protocols":[]},{"id":"582913","is_changed":0,"original_id":"0","is_skipped":"0","is_checked":"0","guid":"148BA898BA88401C9AF688FE43537F88","previous_guid":"BEE82A1D2DFE4924BE7611119066C56B","previous_id":"582912","last_modified":"1502449731","components":[{"component_id":"969831","previous_id":0,"original_id":"0","guid":"1E46030EB7F849EE81283A43D1A52BB7","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>The mean image is subtracted from all data during training and testing in order to improve accuracy on small networks. It ensures that the iamge data is centred around 0, rather than 128 (which is standard for 8 bit images). The mean file can be created using the compute mean executable that ships with caffe:<\/p>\n<pre class=\"language-markup\"><code>\/caffe\/install\/dir\/compute_image_mean .\/data\/db1_training db1.mean.binaryproto<\/code><\/pre>\n<p>This will compute the average colour of all pixels within the db1_training database, and store this in a binaryproto file.<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>The mean image is subtracted from all data during training and testing in order to improve accuracy on small networks. It ensures that the iamge data is centred around 0, rather than 128 (which is standard for 8 bit images). The mean file can be created using the compute mean executable that ships with caffe:<\/p>\n<pre class=\"language-markup\"><code>\/caffe\/install\/dir\/compute_image_mean .\/data\/db1_training db1.mean.binaryproto<\/code><\/pre>\n<p>This will compute the average colour of all pixels within the db1_training database, and store this in a binaryproto file.<\/p>"},"is_project":0},{"component_id":"969832","previous_id":"969831","original_id":"0","guid":"85570BA71CA743C69951D5D1ECC04099","previous_guid":"1E46030EB7F849EE81283A43D1A52BB7","component_type_id":"6","data_id":"0","data":"Create a mean image file","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Create a mean image file"},"is_project":0},{"component_id":"969834","previous_id":"969832","original_id":"0","guid":"67C6CD286C4847F5B3811E2FEDEEF253","previous_guid":"85570BA71CA743C69951D5D1ECC04099","component_type_id":"17","data_id":"1156","data":"<p>A mean binaryproto file that stores the average colour for all images in the training set.<\/p>","order_id":"2","name":"Expected result","data_by_id":"1","type_id":"17","source_data":{"result":"<p>A mean binaryproto file that stores the average colour for all images in the training set.<\/p>"},"is_project":0}],"available_protocols":[]},{"id":"582914","is_changed":0,"original_id":"0","is_skipped":"0","is_checked":"0","guid":"5B0EDB2EAFCE4BE4A877F7CAB246FAE4","previous_guid":"148BA898BA88401C9AF688FE43537F88","previous_id":"582913","last_modified":"1502449898","components":[{"component_id":"969835","previous_id":0,"original_id":"0","guid":"B14B005099B04EEBAD7FE81FDE1FA370","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>Training in caffe can be achieved using the command prompt, by passing a solver file, which in turn refers to a model file defining the CNN architecture. The model file references the training and testing databases, as well as the mean file.<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu9hewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu9hewn.png\" data-ofn=\"p3.png\" \/><\/p>\n<p>Training can be run by calling the caffe executable:\u00a0<\/p>\n<pre class=\"language-markup\"><code>\/caffe\/install\/dir\/caffe train --solver=.\/torch_solver.prototxt --gpu 0<\/code><\/pre>\n<p>The training should run, produce a log of the ongoing training and testing accuracy.<\/p>\n<pre class=\"language-markup\"><code>...\nNetwork initialization done.\nSolver scaffolding done.\nStarting Optimization\n Solving Torch\n Learning Rate Policy: step\n Iteration 0, Testing net (#0)\n     Test net output #0: accuracy = 0.658926\n     Test net output #1: class = 1\n     Test net output #2: class = 0\n     Test net output #3: loss = 0.687874 (* 1 = 0.687874 loss)\n Iteration 0 (-1.4173e-38 iter\/s, 23.9718s\/100 iters), loss = 0.682398\n     Train net output #0: loss = 0.682398 (* 1 = 0.682398 loss)\n Iteration 100 (2.29117 iter\/s, 43.6459s\/100 iters), loss = 0.638753\n     Train net output #0: loss = 0.638753 (* 1 = 0.638753 loss)\n Iteration 200 (2.27664 iter\/s, 43.9244s\/100 iters), loss = 0.612924\n     Train net output #0: loss = 0.612924 (* 1 = 0.612924 loss)\n Iteration 300 (2.26743 iter\/s, 44.1027s\/100 iters), loss = 0.565217\n     Train net output #0: loss = 0.565217 (* 1 = 0.565217 loss)\n Iteration 400 (2.25196 iter\/s, 44.4058s\/100 iters), loss = 0.620955\n     Train net output #0: loss = 0.620955 (* 1 = 0.620955 loss)\n Iteration 500 (2.25102 iter\/s, 44.4244s\/100 iters), loss = 0.568278\n     Train net output #0: loss = 0.568278 (* 1 = 0.568278 loss)\n Iteration 600 (2.25144 iter\/s, 44.416s\/100 iters), loss = 0.540305\n     Train net output #0: loss = 0.540305 (* 1 = 0.540305 loss)\n Iteration 700 (2.25164 iter\/s, 44.4122s\/100 iters), loss = 0.504978\n     Train net output #0: loss = 0.504978 (* 1 = 0.504978 loss)\n Iteration 800 (2.25089 iter\/s, 44.4269s\/100 iters), loss = 0.497804\n     Train net output #0: loss = 0.497804 (* 1 = 0.497804 loss)\n Iteration 900 (2.25158 iter\/s, 44.4133s\/100 iters), loss = 0.477872\n     Train net output #0: loss = 0.477872 (* 1 = 0.477872 loss)\n Iteration 1000, Testing net (#0)\n     Test net output #0: accuracy = 0.909238\n     Test net output #1: class = 0.977556\n     Test net output #2: class = 0.777178\n     Test net output #3: loss = 0.345742 (* 1 = 0.345742 loss)\n...<\/code><\/pre>\n<p>Note that the loss (error) is decreasing and the accuracy between the test at iteration 0 and test at iteration 1000 is greatly improved. Caffe will snapshot trained network weights into a caffemodel file.<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>Training in caffe can be achieved using the command prompt, by passing a solver file, which in turn refers to a model file defining the CNN architecture. The model file references the training and testing databases, as well as the mean file.<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu9hewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mu9hewn.png\" data-ofn=\"p3.png\" \/><\/p>\n<p>Training can be run by calling the caffe executable:\u00a0<\/p>\n<pre class=\"language-markup\"><code>\/caffe\/install\/dir\/caffe train --solver=.\/torch_solver.prototxt --gpu 0<\/code><\/pre>\n<p>The training should run, produce a log of the ongoing training and testing accuracy.<\/p>\n<pre class=\"language-markup\"><code>...\nNetwork initialization done.\nSolver scaffolding done.\nStarting Optimization\n Solving Torch\n Learning Rate Policy: step\n Iteration 0, Testing net (#0)\n     Test net output #0: accuracy = 0.658926\n     Test net output #1: class = 1\n     Test net output #2: class = 0\n     Test net output #3: loss = 0.687874 (* 1 = 0.687874 loss)\n Iteration 0 (-1.4173e-38 iter\/s, 23.9718s\/100 iters), loss = 0.682398\n     Train net output #0: loss = 0.682398 (* 1 = 0.682398 loss)\n Iteration 100 (2.29117 iter\/s, 43.6459s\/100 iters), loss = 0.638753\n     Train net output #0: loss = 0.638753 (* 1 = 0.638753 loss)\n Iteration 200 (2.27664 iter\/s, 43.9244s\/100 iters), loss = 0.612924\n     Train net output #0: loss = 0.612924 (* 1 = 0.612924 loss)\n Iteration 300 (2.26743 iter\/s, 44.1027s\/100 iters), loss = 0.565217\n     Train net output #0: loss = 0.565217 (* 1 = 0.565217 loss)\n Iteration 400 (2.25196 iter\/s, 44.4058s\/100 iters), loss = 0.620955\n     Train net output #0: loss = 0.620955 (* 1 = 0.620955 loss)\n Iteration 500 (2.25102 iter\/s, 44.4244s\/100 iters), loss = 0.568278\n     Train net output #0: loss = 0.568278 (* 1 = 0.568278 loss)\n Iteration 600 (2.25144 iter\/s, 44.416s\/100 iters), loss = 0.540305\n     Train net output #0: loss = 0.540305 (* 1 = 0.540305 loss)\n Iteration 700 (2.25164 iter\/s, 44.4122s\/100 iters), loss = 0.504978\n     Train net output #0: loss = 0.504978 (* 1 = 0.504978 loss)\n Iteration 800 (2.25089 iter\/s, 44.4269s\/100 iters), loss = 0.497804\n     Train net output #0: loss = 0.497804 (* 1 = 0.497804 loss)\n Iteration 900 (2.25158 iter\/s, 44.4133s\/100 iters), loss = 0.477872\n     Train net output #0: loss = 0.477872 (* 1 = 0.477872 loss)\n Iteration 1000, Testing net (#0)\n     Test net output #0: accuracy = 0.909238\n     Test net output #1: class = 0.977556\n     Test net output #2: class = 0.777178\n     Test net output #3: loss = 0.345742 (* 1 = 0.345742 loss)\n...<\/code><\/pre>\n<p>Note that the loss (error) is decreasing and the accuracy between the test at iteration 0 and test at iteration 1000 is greatly improved. Caffe will snapshot trained network weights into a caffemodel file.<\/p>"},"is_project":0},{"component_id":"969836","previous_id":"969835","original_id":"0","guid":"84492368E537498BB0F413B4FB72A739","previous_guid":"B14B005099B04EEBAD7FE81FDE1FA370","component_type_id":"6","data_id":"0","data":"Train the network","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Train the network"},"is_project":0},{"component_id":"969840","previous_id":"969836","original_id":"0","guid":"950AD47203424CBA861C1DA1DB56B1F0","previous_guid":"84492368E537498BB0F413B4FB72A739","component_type_id":"17","data_id":"1157","data":"<p>A .caffemodel file containing the weights of the trained network.<\/p>","order_id":"2","name":"Expected result","data_by_id":"1","type_id":"17","source_data":{"result":"<p>A .caffemodel file containing the weights of the trained network.<\/p>"},"is_project":0}],"available_protocols":[]},{"id":"582915","is_changed":0,"original_id":"0","is_skipped":"0","is_checked":"0","guid":"D4ED096A862343B690254FAA682D88CD","previous_guid":"5B0EDB2EAFCE4BE4A877F7CAB246FAE4","previous_id":"582914","last_modified":"1502450623","components":[{"component_id":"969837","previous_id":0,"original_id":"0","guid":"F758EADAC14A42E787AEA62D9F80DADC","previous_guid":null,"component_type_id":"1","data_id":null,"data":"<p>The heatmap.batch.py file will load the network along with pre-trained weights, and scan an input image at regular intervals. This will be used to create a numpy array (.npy file) that containes the likelihood of each class for each pixel. The render_heatmap.py script will take an input image and combine it with this npy file into a coloured output for visualisation. E.g.<\/p>\n<pre class=\"language-markup\"><code>python heatmap.batch.py .\/image.jpg .\/image.npy<\/code><\/pre>\n<p>Creates an array containing the class likelihoods, then:<\/p>\n<pre class=\"language-markup\"><code>python render_heatmap.py .\/image.jpg .\/image.npy .\/output.jpg<\/code><\/pre>\n<p>Will create an output file similar to these:<\/p>\n<p>\u00a0<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mvahewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mvahewn.png\" data-ofn=\"1.png\" \/><\/p>\n<p>\u00a0<\/p>\n<p>Note: The .sh files within the testing directories can help scan multiple files using this approach, rather than one at a time.<\/p>","order_id":"0","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>The heatmap.batch.py file will load the network along with pre-trained weights, and scan an input image at regular intervals. This will be used to create a numpy array (.npy file) that containes the likelihood of each class for each pixel. The render_heatmap.py script will take an input image and combine it with this npy file into a coloured output for visualisation. E.g.<\/p>\n<pre class=\"language-markup\"><code>python heatmap.batch.py .\/image.jpg .\/image.npy<\/code><\/pre>\n<p>Creates an array containing the class likelihoods, then:<\/p>\n<pre class=\"language-markup\"><code>python render_heatmap.py .\/image.jpg .\/image.npy .\/output.jpg<\/code><\/pre>\n<p>Will create an output file similar to these:<\/p>\n<p>\u00a0<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mvahewn.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/mvahewn.png\" data-ofn=\"1.png\" \/><\/p>\n<p>\u00a0<\/p>\n<p>Note: The .sh files within the testing directories can help scan multiple files using this approach, rather than one at a time.<\/p>"},"is_project":0},{"component_id":"969838","previous_id":"969837","original_id":"0","guid":"257E840D8B744AE096D530C83C7CF1E6","previous_guid":"F758EADAC14A42E787AEA62D9F80DADC","component_type_id":"6","data_id":"0","data":"Use the trained weights to scan over whole input images","order_id":"1","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Use the trained weights to scan over whole input images"},"is_project":0},{"component_id":"969841","previous_id":"969838","original_id":"0","guid":"F7D42073A46B4F0FA3E15A23459E185C","previous_guid":"257E840D8B744AE096D530C83C7CF1E6","component_type_id":"17","data_id":"1158","data":"<p>Output arrays and images locating features of interest.<\/p>","order_id":"2","name":"Expected result","data_by_id":"1","type_id":"17","source_data":{"result":"<p>Output arrays and images locating features of interest.<\/p>"},"is_project":0}],"available_protocols":[]}]}