{"id":17729,"title":"Default Fuhrman Lab Pipeline for Exact Amplicon Sequence Variant (eASV) Calling from SSU rRNA Amplicons Derived From 515Y\/926R Primers","title_html":"Default Fuhrman Lab Pipeline for Exact Amplicon Sequence Variant (eASV) Calling from SSU rRNA Amplicons Derived From 515Y\/926R Primers","image":{"source":"https:\/\/www.protocols.io\/img\/default_protocol.png","placeholder":"https:\/\/www.protocols.io\/img\/default_protocol.png"},"doi":"dx.doi.org\/10.17504\/protocols.io.vi9e4h6","doi_status":2,"uri":"default-fuhrman-lab-pipeline-for-exact-amplicon-se-vi9e4h6","type_id":1,"published_on":1570656843,"parent_protocols":[],"parent_collections":[],"version_id":0,"created_on":1542052783,"categories":null,"creator":{"name":"Jed Fuhrman","affiliation":"University of Southern California","affiliations":[{"affiliation":"University of Southern California","url":"http:\/\/fuhrmanlab@gmail.com","is_default":1}],"username":"jed-fuhrman","link":"https:\/\/onlinelibrary.wiley.com\/doi\/full\/10.1111\/1462-2920.13023","image":{"source":"\/img\/avatars\/008.png","placeholder":"\/img\/avatars\/008.png"},"badges":[{"id":4,"image":{"source":"\/img\/badges\/gold.svg","placeholder":"\/img\/badges\/gold.svg"},"name":"Gold power author!"},{"id":5,"image":{"source":"\/img\/badges\/earlyadopter.svg","placeholder":"\/img\/badges\/earlyadopter.svg"},"name":"Early adopter"},{"id":6,"image":{"source":"\/img\/badges\/socialbutterfly.svg","placeholder":"\/img\/badges\/socialbutterfly.svg"},"name":"Social butterfly"}],"research_interests":null,"blocked_by_you":false,"blocked_you":false},"journal":null,"journal_name":null,"journal_link":null,"article_citation":null,"public":1,"has_versions":0,"link":"https:\/\/onlinelibrary.wiley.com\/doi\/full\/10.1111\/1462-2920.13023","total_collections":0,"number_of_steps":18,"authors":[{"name":"Jesse McNichol","affiliation":"University of Southern California","affiliations":[],"username":"jesse-mcnichol","link":null,"image":{"source":"\/img\/avatars\/004.png","placeholder":"\/img\/avatars\/004.png"},"badges":[],"research_interests":null,"blocked_by_you":false,"blocked_you":false}],"versions":[],"groups":[],"has_subprotocols":0,"is_subprotocol":0,"is_bookmarked":0,"can_be_copied":1,"can_remove_fork":1,"forks_count":{"private":0,"public":0},"access":{"can_view":1,"can_remove":0,"can_add":0,"can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":1,"can_move":1,"can_move_outside":1,"can_transfer":1,"can_download":1,"is_locked":0},"guid":"87455405B9DA11E99CA40242AC110005","steps":[{"id":682256,"guid":"BDA53DCCF4494A318C4A4B837B74820F","previous_id":null,"previous_guid":null,"modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"3399099A4B584C5381EB89F2170C53C8","order_id":1,"type_id":6,"title":"Section","source":{"title":"Removing primers and sorting 16s and 18s into two separate pools"}},{"id":1054724,"guid":"BA01A0816DC9462595C29D46EF3D62DF","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\">Make sure you have the correct environment configuration for conda by typing \"which conda\" in your terminal. It should output \"\/home\/anaconda\/miniconda2\/bin\/conda\". If not, please refer to Guidelines and Warnings to get set up properly, or install on your own environment if you so desire.<\/li><li style = \"counter-reset:ol0;\">Make a folder with an informative name (hereafter referred to as the \"base directory\").<\/li><li style = \"counter-reset:ol0;\">Make a README inside the folder with information for posterity (at the same level of detail you'd put in your lab notebook).<\/li><li style = \"counter-reset:ol0;\">Put your raw, demultiplexed files in a folder called \"00-raw\" within your base directory.<\/li><li style = \"counter-reset:ol0;\"><span>Format should be: forward =<\/span><span style = \"font-weight:bold;\"> \"00-raw\/*.R1.fastq.gz\"  reverse = \"00-raw\/*.R2.fastq.gz\",<\/span><span> rename\/gzip as necessary. Don't just rename to gz, otherwise you will have problems later. Running <\/span><span style = \"font-weight:bold;\">\"gzip *\"<\/span><span> in your directory will do the trick. Use \"pigz\" (parallel gzip) if you're in a hurry. FYI you can always check if a file is gzipped for real by going \"<\/span><span style = \"font-weight:bold;\">head yourfile<\/span><span>\". If it's just named .gz but not gzipped, it will appear as plaintext. Otherwise it will look like gibberish.<\/span><\/li><li style = \"counter-reset:ol0;\">Make sure you have access to the necessary scripts, clone from github as follows (The pipeline may be improved\/modified occasionally, so it might be wise to do this every time you start a new analysis. I would also keep copies of the scripts after running them in your working directory, that way you'll be able to exactly reproduce your analysis later if need be or understand what you did later.): <\/li><\/ol><\/div><div class = \"text-block\"><pre><code><div class = \"text-blocks\"><div class = \"text-block\"><a href=\"#\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">git clone https:\/\/github.com\/jcmcnch\/eASV-pipeline-for-515Y-926R.git<\/span><\/a><\/div><div class = \"text-block\"><pre><code>git clone https:\/\/github.com\/jcmcnch\/eASV-pipeline-for-515Y-926R.git<\/code><\/pre><\/div><div class = \"text-block\">The github repository is now structured into a single \"master\" branch that contains 3 variants of the pipeline, each in its own folder (with subfolders for the different parts of the pipeline).<\/div><\/div><\/code><\/pre><\/div><div class = \"text-block\">7. You'll be using the scripts in the \"DADA2-pipeline\" subfolder. Make scripts executable (you can just do this: \"chmod a+x .\/*\" in the cloned directory)<\/div><div class = \"text-block\">8. Confirm they're executable (check by doing \"ls -la\")<\/div><div class = \"text-block\">9. If desired, add the scripts to your $PATH variable so they will be accessible everywhere when you login (see example below):<\/div><div class = \"text-block\"><pre><code><div class = \"text-blocks\"><div class = \"text-block\">#add the following line to your ~\/.bashrc (can do it in the terminal with \"vi\" or with a GUI editor as you prefer)<\/div><div class = \"text-block\">export PATH=\"\/path\/to\/dir:$PATH\"<\/div><div class = \"text-block\">#now \"refresh\" your ~\/.bashrc so the changes take effect (do this in the terminal)<\/div><div class = \"text-block\">source ~\/.bashrc<\/div><\/div><\/code><\/pre><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#A492FF","critical":null,"critical_id":null,"duration":0},{"id":682257,"guid":"F025A08F0E5A4F98B784BCCFF5D49FBA","previous_id":682256,"previous_guid":"BDA53DCCF4494A318C4A4B837B74820F","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"36F3ABD28D9545B2A28DA3FEB46B3798","order_id":1,"type_id":6,"title":"Section","source":{"title":"Removing primers and sorting 16s and 18s into two separate pools"}},{"id":1054724,"guid":"7AC023C638E547BBBD50BF527AD5B70C","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run the script \"<\/span><span style = \"font-weight:bold;\">00-run-cutadapt.sh<\/span><span>\" (found in the \"00-trimming-sorting-scripts\" subdirectory). The script itself can be anywhere, but it must be called from the directory containing the folder \"<\/span><span style = \"font-weight:bold;\">00-raw<\/span><span style = \"font-weight:bold;\">\"<\/span><\/li><li style = \"counter-reset:ol0;\">Check logs to make sure the number of reads with primers being identified is very high (close to 100%)<\/li><li style = \"counter-reset:ol0;\">Look at the output files, make sure the sizes are reasonable<\/li><li style = \"counter-reset:ol0;\">If script fails, make sure the naming format is correct as specified above and the script is executable<\/li><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#A492FF","critical":null,"critical_id":null,"duration":0},{"id":682258,"guid":"D14A570342CA471AB1821D8E759FCBEA","previous_id":682257,"previous_guid":"F025A08F0E5A4F98B784BCCFF5D49FBA","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"533CEEE83E8A44EFB134098FDF917D19","order_id":1,"type_id":6,"title":"Section","source":{"title":"Removing primers and sorting 16s and 18s into two separate pools"}},{"id":1054724,"guid":"29B29D3DDCCE41B3BF722D89C4791C52","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run the script \"<\/span><span style = \"font-weight:bold;\">01-sort-16S-18S-bbsplit.sh<\/span><span>\" from your base directory. This will split your mixed reads into 16s and 18s. It also copies some \"in-silico mocks\" that Liv generated into the analysis. These will come in handy later when you're QC'ing your pipeline.<\/span><\/li><li style = \"counter-reset:ol0;\">Check the logs, make sure a very high proportion of reads are sorted (close to 100%, should be 100% for the mocks).<\/li><li style = \"counter-reset:ol0;\">Proceed to individual procedures for each fraction (PROK and EUK)<\/li><\/ol><\/div><\/div>"}}],"cases":[{"guid":"92CF1B36B9DA11E99CA40242AC110005","step_guid":"D14A570342CA471AB1821D8E759FCBEA","title":"For PROK (16S) sequences","short_title":null,"description":"{\"blocks\":[{\"key\":\"5vbn2\",\"text\":\"Now that you've cut your primers and sorted your sequences, the steps below are the pipeline for processing the PROK sequences sorted in the previous step. There is no need to run this prior to the EUK analysis - e.g. you could run one or both at the same time.\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[],\"data\":{}}],\"entityMap\":{}}","first_step_id":682259,"first_step_guid":"BC81E8658E334233A4DA580E8153465F"}],"data":null,"section":null,"section_color":"#A492FF","critical":null,"critical_id":null,"duration":0},{"id":682259,"guid":"BC81E8658E334233A4DA580E8153465F","previous_id":682258,"previous_guid":"D14A570342CA471AB1821D8E759FCBEA","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"81EAFE02620646ED9465FD7F74881FCE","order_id":1,"type_id":6,"title":"Section","source":{"title":"Get set up"}},{"id":1054724,"guid":"161D32DB75A04FCBAE672A61E4F0325F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>1.  Go into your \"<\/span><span style = \"font-weight:bold;\">02-PROKs<\/span><span>\" directory. All scripts for the PROK part of the pipeline (prefaced with \"P\") should be run from here from now on (they can be found in the \"01-prok-scripts\" subfolder). One thing I like to do is create a \"scripts\" folder in this directory and then copy the P* scripts here. That way you can just run \".\/scripts\/your-step.sh\". <\/span><\/div><div class = \"text-block\"><span>2.  Look at the files in<\/span><span style = \"font-weight:bold;\"> 02-PROKs\/00-fastq<\/span><span>. Make sure they are a reasonable size. If there are mostly 0 size files, then something must have gone wrong with the sorting. Check to make sure you are sorting the correct things. In the PROK sequences, you will have some 0-sized files but they should be rare (i.e. pure EUK mocks).<\/span><\/div><div class = \"text-block\"><span>3. Make your manifest. You can do this manually, but I've got a shell script that does the job, which also removes empty files in the process: \"<\/span><span style = \"font-weight:bold;\">P00-create-manifest.sh<\/span><span>\". Just run it from your 02-PROKs folder and double-check the resulting csv file in libreoffice\/excel\/gnumeric to make sure it looks right. <\/span><span style = \"font-weight:bold;\">Any mistakes in your manifest will be carried throughout the rest of the analysis pipeline, so make sure the sample names are all correct! NOTE: Sample ID names should be less than 32 characters. <\/span><span>[Why? The sample-metadata file used to make interactive barplots (Step 11) requires sample ID names less than 32 characters. The Sample ID of the sample-metadata file and the Sample ID of this manifest file must match, so might as well shorten the ID names of the manifest file at this current step. The sample ID names will be used as labels on the barplots].<\/span><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#94EBFF","critical":null,"critical_id":null,"duration":0},{"id":682260,"guid":"18F2D4B9C65542FD94792FBEACD9B4B4","previous_id":682259,"previous_guid":"BC81E8658E334233A4DA580E8153465F","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"CBD90C6654AE4FC08FBBB560CC37C92E","order_id":1,"type_id":6,"title":"Section","source":{"title":"Import your sequences into a qiime2 artifact"}},{"id":1054724,"guid":"8924208BC00840A484AF4232298D44B2","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#84CE84","critical":null,"critical_id":null,"duration":0},{"id":682261,"guid":"62ADF1CACAEE40E49F723ECB11DDA30B","previous_id":682260,"previous_guid":"18F2D4B9C65542FD94792FBEACD9B4B4","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"79AB29A1B52342D2A0E79FF6BE5A371C","order_id":1,"type_id":6,"title":"Section","source":{"title":"Visualize sequence quality"}},{"id":1054724,"guid":"B0509EC82CB047699BE910479434ADC0","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run the script \"<\/span><span style = \"font-weight:bold;\">P02-visualize-quality_R1-R2.sh<\/span><span>\". This creates another artifact, this time of the visualization flavor.<\/span><\/li><li style = \"counter-reset:ol0;\">Now, visualize by opening up the folder on kraken containing the artifact (if this is not possible, you can always <\/li><li style = \"counter-reset:ol0;\"><a href=\"https:\/\/haydenjames.io\/linux-securely-copy-files-using-scp\/\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">scp<\/span><\/a><\/li><li style = \"counter-reset:ol0;\"> the file onto your desktop)<\/li><li style = \"counter-reset:ol0;\"><span>You will be using this visualization to decide how much of your R1 and R2 sequences to keep.This is a balancing act between keeping enough of the R2 read (which decreases in quality quite rapidly) to allow merging but not so much that DADA2 will start calling spurious things or discarding your sequences because of with poor-quality data. In my experience, keeping some of the R2 where the quality starts to spread a lot is OK, but <\/span><span style = \"font-weight:bold;\">keep the average sequence quality<\/span><span> (i.e. the line in the middle of the box plot) <\/span><span style = \"font-weight:bold;\">above 30<\/span><span>. You also need <\/span><span style = \"font-weight:bold;\">at least 20 nt overlap<\/span><span> for the merging step. Don't fret too much though, as you can check later with the DADA2 output & mocks to know whether or not your trim length is appropriate.<\/span><\/li><li style = \"counter-reset:ol0;\">Make a note of these values in your computational notebook for the next step.<\/li><\/ol><\/div><div class = \"text-block\">Now, visualize by opening up the folder on kraken containing the artifact (if this is not possible, you can always <\/div><div class = \"text-block\"><a href=\"https:\/\/haydenjames.io\/linux-securely-copy-files-using-scp\/\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">scp<\/span><\/a><\/div><div class = \"text-block\"> the file onto your desktop)<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>You will be using this visualization to decide how much of your R1 and R2 sequences to keep.This is a balancing act between keeping enough of the R2 read (which decreases in quality quite rapidly) to allow merging but not so much that DADA2 will start calling spurious things or discarding your sequences because of with poor-quality data. In my experience, keeping some of the R2 where the quality starts to spread a lot is OK, but <\/span><span style = \"font-weight:bold;\">keep the average sequence quality<\/span><span> (i.e. the line in the middle of the box plot) <\/span><span style = \"font-weight:bold;\">above 30<\/span><span>. You also need <\/span><span style = \"font-weight:bold;\">at least 20 nt overlap<\/span><span> for the merging step. Don't fret too much though, as you can check later with the DADA2 output & mocks to know whether or not your trim length is appropriate.<\/span><\/li><li style = \"counter-reset:ol0;\">Make a note of these values in your computational notebook for the next step.<\/li><\/ol><\/div><div class = \"text-block\"><span class = \"embed-element\"><\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8jqi82e.gif\" \/><\/div><\/div>"}},{"id":1054725,"guid":"64E144628A054DBABEE7F2A897107D3E","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8jqi82e.gif\" \/><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#FFED92","critical":null,"critical_id":null,"duration":0},{"id":682266,"guid":"C51E0BF803B248CAA1D938E638AAD2A9","previous_id":682317,"previous_guid":"EAA4B6E0E6C111E8804139BD74755675","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"4DEC3B6C1CE94E5C9295EAD302A910F9","order_id":1,"type_id":6,"title":"Section","source":{"title":"Export DADA2 results"}},{"id":1054724,"guid":"BB115183F3124905A62029F97EFF8915","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run the script \"<\/span><span style = \"font-weight:bold;\">P04-export-DADA2-results.sh<\/span><span>\". This exports various things from the artifacts into human-readable files. Don't use these for downstream processing, they're just being exported to look at them and make sure everything went well.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>First, check the \"stats.tsv\" file found in <\/span><span style = \"font-weight:bold;\">03-DADA2d<\/span><span> and make sure most of your sequences make it through the pipeline. If not, you may have over-trimmed and may have to readjust your trim lengths.<\/span><\/li><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#E57785","critical":null,"critical_id":null,"duration":0},{"id":682267,"guid":"C5053E3085734F70BDD66705147A823B","previous_id":682318,"previous_guid":"C53997D0E6C211E8804139BD74755675","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"74C7CDD9589B4E1CB21869F362232BB2","order_id":1,"type_id":6,"title":"Section","source":{"title":"Classify eASVs against SILVA"}},{"id":1054724,"guid":"D567A4D5C26B445B9C2BB41FB29DC88F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span>Once you're confident that your mocks look good, and that you don't have any major issues with your denoising, run the script \"<\/span><span style = \"font-weight:bold;\">P05-classify-eASVs.sh<\/span><span> \". This assigns taxonomy to your reads based on a classifier derived from the SILVA 132 database (sliced to the region between our default primers). You can read more about it <\/span><\/div><div class = \"text-block\"><a href=\"https:\/\/microbiomejournal.biomedcentral.com\/articles\/10.1186\/s40168-018-0470-z\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">here<\/span><\/a><\/div><div class = \"text-block\">. As far as I can tell, it relies on k-mer based classification and so should be appropriate for both prokaryotic and eukaryotic sequences.<\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#A492FF","critical":null,"critical_id":null,"duration":0},{"id":682268,"guid":"2ADC39DEB3D847B8B57CEAE636D7471C","previous_id":682267,"previous_guid":"C5053E3085734F70BDD66705147A823B","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"398FC556817A409FA2F4D1721B2DE7FE","order_id":1,"type_id":6,"title":"Section","source":{"title":"Visualize the results with qiime2's built-in interactive barplots"}},{"id":1054724,"guid":"64B9D2C105F848DA971C218A27AEF632","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>You first need to make a sample metadata file, which contains any relevant information about your samples that might be useful in plotting. e.g. date, size fraction, etc. Run the following command \"<\/span><span style = \"font-weight:bold;\">P06-make-sample-metadata-file.py manifest.csv > sample-metadata.tsv<\/span><span>\". This will create a template sample metadata file for you. Open it up in your favourite editor and paste in the metadata. Make sure you save it afterwards in the plaintext tab-separated format. Be careful that you've put your metadata in correctly (this is one of those manual steps that is prone to human error).<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Now run \"<\/span><span style = \"font-weight:bold;\">P07-make-barplot.sh<\/span><span> \", which will take your abundance information, metadata, and taxonomy and plot it out nicely.<\/span><\/li><li style = \"counter-reset:ol0;\">Visualize as before, making sure the results look reasonable. Make sure the top taxa are what you expect. Look for potential contaminants or things that do not belong. Spend some time fussing around with it, sorting by metadata etc:<\/li><\/ol><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8nfi82e.gif\" \/><\/div><\/div>"}},{"id":1054725,"guid":"EA81AB9554CB467381783DA7BF7F7A53","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8nfi82e.gif\" \/><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#94EBFF","critical":null,"critical_id":null,"duration":0},{"id":682269,"guid":"705716F9F52240499CDFA9A21DFA453C","previous_id":682270,"previous_guid":"8980E7133CCB4B6D86D32CD040DF4062","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"23F36396EE8C4433812AF27A929FECB3","order_id":1,"type_id":6,"title":"Section","source":{"title":"Export and subset data for further analysis"}},{"id":1054724,"guid":"B5DB5357E815445F96E25AA2705B22B5","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">In many cases, you will want to export the data for further analysis or plotting. I like to ouptut the data in a plaintext format (i.e. tab-separated spreadsheet), which can be easily visualized in a spreasheet program or imported into other software like R\/matlab etc. It's also helpful to split the tables into various functional categories (e.g. algae only, heterotrophic prokaryotes only without mitochondria\/chloroplasts). These splitting and exporting steps are a little convoluted with qiime2, so I've written some scripts to automate this.<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>First, run \"<\/span><span style = \"font-weight:bold;\">P09-split-mito-chloro-PhytoRef-reclassify.sh\"<\/span><span> which will split out your Chloroplast sequences (based off of initial classifications by SILVA132), reclassify them with PhytoRef, and then generate a merged taxonomy that has SILVA132 classifications for all 16S sequences except for PhytoRef classifications for chloroplasts. <\/span><span style = \"font-weight:bold;\">NB-The PhytoRef default Chloroplast taxonomy starts with \"Kingdom:Eukaryota\" so be aware of that when you look at your exported tables\/plots.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Next, run \"<\/span><span style = \"font-weight:bold;\">P10-generate-tsv-biom-tables-with-taxonomy.sh<\/span><span>\", which will export your biom table from the artifact, add the taxonomy information, and then export the whole thing as a .tsv file. It will do this export for every folder in which it finds a feature-table.qza, so it works in batch if you have clustered your eASVs to make OTUs.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Check the output in <\/span><span style = \"font-weight:bold;\">10-exports\/ <\/span><span>to make sure everything looks reasonable. Look at the mock community if you have it.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Now, the tab-separated file that has been output is in the format of counts. Proportions are typically more informative, so you can run the script \"<\/span><span style = \"font-weight:bold;\">P10-transform-tsv-to-proportions.sh<\/span><span>\" which runs a python script (which needs to be in your $PATH or in the working directory) that converts your table to proportions and optionally filters out low-abundance eASVs (as with OTUs, there is a long tail of not-very-abundant things). You can get more information about how it works by running \"<\/span><span style = \"font-weight:bold;\">transform-ESV-tsv-to-proportions.py -h<\/span><span>\". By default, the settings I've provided in the shell script (that runs this python script) will filter out eASVs that are always less than 0.01 % and also keep only the set of eASVs that allow for 99% of the abundance to be recovered in all samples (all of these produce different spreadsheet outputs).<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Next, you can run \"<\/span><span style = \"font-weight:bold;\">P12-make-subsetted-barplots.sh<\/span><span>\" which will generate barplots with the updated PhytoRef-containing taxonomy (the first barplot in the P07 step above is only SILVA132), and all of the different subsets that you might want to look at. For example, you could look at the community without any chloroplasts or mitochondria, or just the chloroplasts, or just the mitochondria (among other options that you can see in the \"10-exports\" folder).<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Finally, if there are certain samples that you would like to exclude from your newly subsetted barplots, like mocks or replicates for example, run \"<\/span><span style = \"font-weight:bold;\">P13-exclude-samples-from-barplots.sh\". <\/span><span>You must first manually generate a sample-to-keep.tsv file which lists the sample IDs that you want to include in all of your barplots. Once generated, the P13 script will continue. <\/span><\/li><\/ol><\/div><\/div>"}}],"cases":[{"guid":"92D0BBF6B9DA11E99CA40242AC110005","step_guid":"705716F9F52240499CDFA9A21DFA453C","title":"For EUK sequences","short_title":null,"description":"{\"blocks\":[{\"key\":\"def6h\",\"text\":\"Now we analyze the EUK sequences separately. Since these sequences do not overlap, we have to use a different pre-processing workflow which is nearly identical to what was previously described. Then these pre-processed sequences are put into qiime2 in a similar manner to the PROK sequences.\\n\\nWARNING: As with the PROK pipeline, you must verify that the real mock sequences come out in the same row as the in-silico mocks and that they are all present. If you trim inappropriately (i.e. keep too much bad sequence in the R2) or mix runs, you may encounter missing mock sequences or spurious eASVs (sometimes caused by the sequences getting truncated at the beginning of the bad quality region). If for whatever reason you don't have real mocks in your run, you will have to be very careful using DADA2 as it is more sensitive to these sort of errors than deblur. You can probably still use it, but proceed with caution as you will have no way to verify your eASVs are called correctly. Deblur can always be a backup option, but it will discard a very large fraction of your sequences (hence why we are currently recommending DADA2, especially for the analysis of EUK sequences).\",\"type\":\"unstyled\",\"depth\":0,\"inlineStyleRanges\":[],\"entityRanges\":[{\"offset\":172,\"length\":20,\"key\":0}],\"data\":{}}],\"entityMap\":{\"0\":{\"type\":\"link\",\"mutability\":\"MUTABLE\",\"data\":{\"url\":\"https:\/\/www.protocols.io\/view\/fuhrman-lab-515f-926r-16s-and-18s-rrna-sequencing-nkhdct6?step=102\"}}}}","first_step_id":null,"first_step_guid":"24F4A94845754898B3513E0DE75F18B3"}],"data":null,"section":null,"section_color":"#FFED92","critical":null,"critical_id":null,"duration":0},{"id":682270,"guid":"8980E7133CCB4B6D86D32CD040DF4062","previous_id":682268,"previous_guid":"2ADC39DEB3D847B8B57CEAE636D7471C","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"26F838194E6F417BA2345263C0822685","order_id":1,"type_id":6,"title":"Section","source":{"title":"OPTIONAL: Cluster your aESVs to make OTUs"}},{"id":1054724,"guid":"FD61DCF1259A4A2E9828553AF0B3A781","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">You might want to do some OTU clustering with your eASVs but this is strictly optional. qiime2 has a nice way of handling this functionality (with VSEARCH) this and I suggest doing it after the barplot step if you're going to do it. This way, the subsequent scripts will take the information from the clustering and output them as tables along with the original (unclustered spreadsheet). NB: One of the pieces of information we may wish to have later on is the membership of each cluster. For example, if you want to know how many close relatives a particular eASV has, you would have to know which cluster it is in. By default, this information is not retained in qiime2 but I found a <\/div><div class = \"text-block\"><a href=\"https:\/\/forum.qiime2.org\/t\/when-clustering-asvs-from-deblur-with-vsearch-is-it-possible-to-know-the-membership-of-resulting-clusters\/6022\/6\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">workaround<\/span><\/a><\/div><div class = \"text-block\"> with help from the qiime2 peeps. So it is now possible to do this with the scripts I provide (but it depends on a qiime2 conda environment I've hacked - available on kraken).<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run the script \"<\/span><span style = \"font-weight:bold;\">P08-optionally-cluster-eASVs.sh<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\"><desired cluster level in percent><\/span><span>\". e.g. \"<\/span><span style = \"font-weight:bold;\">P08-optionally-cluster-eASVs.sh<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\">99<\/span><span>\" for 99% clustering. You can run this as many times as you want, it will create a new output folder as long as you supply a different clustering level.<\/span><\/li><li style = \"counter-reset:ol0;\">Capture the uc file (if desired) as shown in the image below.<\/li><li style = \"counter-reset:ol0;\"><span>You can later parse this .uc file with the script \"<\/span><span style = \"font-weight:bold;\">parse_VSEARCH_cluster_membership_from_UC_file.py<\/span><span>\" when you need the membership information. See the help function for this script for more info.<\/span><\/li><\/ol><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8nii82e.png\" \/><\/div><\/div>"}},{"id":1054725,"guid":"78E0B6CDCBCB46C1ABDE5FFE0E5B2E97","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8nii82e.png\" \/><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#84CE84","critical":null,"critical_id":null,"duration":0},{"id":682271,"guid":"24F4A94845754898B3513E0DE75F18B3","previous_id":682269,"previous_guid":"705716F9F52240499CDFA9A21DFA453C","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"19E53A0E7C5F423FBAAB99F5C2994D10","order_id":1,"type_id":6,"title":"Section","source":{"title":"Visualize the quality of EUK R1 R2 to define trim lengths."}},{"id":1054724,"guid":"38D3991926A94F00B8C21F036E0DB09D","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Since we cannot rely on the read overlapping step to correct errors (as in the PROK analysis), we must make a judgement call on which part of the read is too ugly to keep. We'll use qiime2's beautiful interactive visualization for this (which means we have to import into an artifact first).<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\">As with the PROK analysis, go into your 02-EUKs folder, and run the remaining scripts from here.<\/li><li style = \"counter-reset:ol0;\"><span>First, create your manifest as before by running \"<\/span><span style = \"font-weight:bold;\">E00-create-manifest-viz.sh<\/span><span style = \"font-weight:bold;\">\"<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Next, import by running \"<\/span><span style = \"font-weight:bold;\">E01-import.sh<\/span><span style = \"font-weight:bold;\">\"<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Visualize by running \"<\/span><span style = \"font-weight:bold;\">E02-visualize-quality_R1-R2.sh<\/span><span>\".<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Now, visualize as before and pick what you think is a reasonable trim length (see gif below for a reminder). This, of course, is somewhat arbitrary. In this gif, I've picked 221 (basically all of the forward read) and 192 (as much of the reverse read as looks reasonable. However, this may vary from dataset to dataset. In some more recent work, I've seen that a shorter R2 trim length may be necessary (i.e. 120), otherwise your mocks are not recovered. <\/span><span style = \"font-weight:bold;\">Whichever trim length you choose, make sure it is consistent within any set of samples that you want to cross-compare and pay close attention to the QC steps where problems may present themselves!<\/span><\/li><\/ol><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8wvi82e.gif\" \/><\/div><\/div>"}},{"id":1054725,"guid":"B7366DC69B374DED99748236EB7D8F3D","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/8wvi82e.gif\" \/><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#EA9F6C","critical":null,"critical_id":null,"duration":0},{"id":682317,"guid":"EAA4B6E0E6C111E8804139BD74755675","previous_id":682261,"previous_guid":"62ADF1CACAEE40E49F723ECB11DDA30B","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"C8AD4626027E4E22982F845A7BD5A939","order_id":1,"type_id":6,"title":"Section","source":{"title":"Run DADA2!"}},{"id":1054724,"guid":"32442A2531394B6B83725BE211BAD87F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><\/ol><\/div><div class = \"text-block\"><pre><code><div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">P03-DADA2.sh trimleft trimright<\/span><\/div><div class = \"text-block\"><span style = \"font-weight:bold;\">e.g.<\/span><\/div><div class = \"text-block\"><span style = \"font-weight:bold;\">$ P03-DAD2.sh 214 187<\/span><\/div><\/div><\/code><\/pre><\/div><div class = \"text-block\"><span>The script will not work unless you put in your trim lengths, and will print them to a file for your future reference. The q2-dada2 plugin is really wrapping a number of separate steps called in the DADA2 pipeline, but is greatly simplified in qiime2 (lucky for us). Liv has tested it inside qiime2 and compared with results from the latest R version of DADA2 and has not noticed any differences. <\/span><span style = \"font-weight:bold;\">Remember that DADA2 constructs a run-specific error model, and so we should analyze runs separately!<\/span><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#EA9F6C","critical":null,"critical_id":null,"duration":0},{"id":682318,"guid":"C53997D0E6C211E8804139BD74755675","previous_id":682266,"previous_guid":"C51E0BF803B248CAA1D938E638AAD2A9","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"F40F0676ABE74C87AB56CE0CD57F11DD","order_id":1,"type_id":6,"title":"Section","source":{"title":"Check DADA2 results to look for spurious eASVs or missing sequences"}},{"id":1054724,"guid":"EB5C51506B2A4B7BB7CC81DE38ADD199","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">This step requires you to look carefully at your eASV\/OTU table:<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Open up your<\/span><span style = \"font-weight:bold;\"> \"feature-table.biom.tsv\"<\/span><span> file (found in <\/span><span style = \"font-weight:bold;\">03-DADA2d<\/span><span>) in a spreadsheet program so you can see it better.<\/span><\/li><li style = \"counter-reset:ol0;\">I usually delete the top row, as it's meaningless.<\/li><li style = \"counter-reset:ol0;\">Then find your in-silico staggered mock, and sort the whole spreadsheet in descending order according to that column.<\/li><li style = \"counter-reset:ol0;\">Now, compare your real staggered mock with your in-silico mock. Each row is a distinct eASV, and so you should have the same number of rows (or less) in your real mock as the in-silico mock (which is your \"positive control\" as they are the exact sequences with perfect quality). Some of the low-abundance eASVs may not be detectable depending on your sequencing depth so that's nothing to worry about. If some of the abundant mocks are missing, this is however a red flag.<\/li><li style = \"counter-reset:ol0;\"><span>Now, you may discover that there are some eASVs in your mocks that don't match up with the in-silico mocks. Don't panic! These can be usually accounted for by sample bleedthrough, but could also be an indication of spurious eASVs being produced by DADA2. We are considering that an eASV is spurious if it has 1-mismatch from the reference sequence across the whole length (we have seen this happen before). In any case, better check. Copy the names of the eASVs into a plaintext file in your<\/span><span style = \"font-weight:bold;\"> 03-DADA2d<\/span><span> folder.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Now grab those sequences. I usually use the command \"<\/span><span style = \"font-weight:bold;\">seqtk subseq dna-sequences.fasta <your file containing questionable eASVs> > questionable-eASVs.fasta<\/span><span>\". FYI dna-sequences.fasta containts the exact sequences of your eASVs and this command just takes a subset of them based on their fasta identifiers.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Next, blast those eASVs against your mocks. I have a simple wrapper script for this, \"<\/span><span style = \"font-weight:bold;\">16s-mock-blast.sh\"<\/span><span>. Run as follows:<\/span><span style = \"font-weight:bold;\"> \"16s-mock-blast.sh questionable-eASVs.fasta ouptut.tsv\"<\/span><\/li><li style = \"counter-reset:ol0;\">Open the output file, and sort by the column \"mismatches\". Look for ones that have only 1 mismatch. These might be spurious eASVs. If these are not present, that indicates that DADA2 is doing what it's supposed to (at least as far as we can tell from the mocks).<\/li><li style = \"counter-reset:ol0;\">Now, if there are some eASVs present that should not be there but are not a close relative of the mock, then it has to have come from somewhere. There are two possibilities - internal sample bleedthrough or external contamination. The latter seems to be quite rare (although it has apparently happened - saw hydrothermal sequences in SPOT samples once), but the former is fairly common. Bleedthrough can be spotted by scrolling across an eASV row. If the questionable eASV is present in high abundance in other real or mock samples, it's a dead ringer for sample bleedthrough. We don't have a clear recommendation to deal with such sequences at this point, but they seem mercifully rare. Contamination by contrast would be those questionable sequences that could not be accounted for by bleedthrough. These may need to be removed.<\/li><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#EA94FF","critical":null,"critical_id":null,"duration":0},{"id":682380,"guid":"30AD6230E6D211E8804139BD74755675","previous_id":682271,"previous_guid":"24F4A94845754898B3513E0DE75F18B3","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"D2519643020747169D072251A632FFFE","order_id":1,"type_id":6,"title":"Section","source":{"title":"Cut and paste"}},{"id":1054724,"guid":"64A54D077E5A42E8A7333C28827B740F","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">In this step, you are cutting your reads to your chosen trim length and then artificially concatenating them.<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E03-bbduk-cut-reads.sh<\/span><span style = \"font-weight:bold;font-weight:bold;\"> <\/span><span style = \"font-weight:bold;\"><fwd trim length> <rev trim length><\/span><span>\" to cut your reads.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E04-fuse-EUKs-withoutNs.sh<\/span><span> \" to fuse them together into one read. This differs from what David et al have done previously where an N was added in the middle of the sequence. Remember that when submitting data for publication, something should be added to indicate that these are artificially-fused sequences. It will not cause issues with denoising or taxonomy assignment with the steps outlined in this protocol, but would cause problems if you were using an alignment-based taxonomic assignment algorithm.<\/span><\/li><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#E57785","critical":null,"critical_id":null,"duration":0},{"id":682384,"guid":"F3113800E6D311E8804139BD74755675","previous_id":682380,"previous_guid":"30AD6230E6D211E8804139BD74755675","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"87EDBEE238AC4B41A29E56B66124D264","order_id":1,"type_id":6,"title":"Section","source":{"title":"Run DADA2"}},{"id":1054724,"guid":"FB38E100D1C741E7B73CB3A7F3FD8014","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Because the last few steps were done outside the qiime2 environment (we only imported at the beginning for visualization), we now have to import as before:<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E05-create-manifest-concat.sh<\/span><span>\" to automatically create your manifest.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E06-import-concat.sh<\/span><span>\" to import your sequences.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E07-visualize-quality-single-seqs.sh<\/span><span>\" to get a visualization artifact for your merged sequences. Take a look as before, you should see a drop-off in the middle (corresponding to the end of the R2) with higher qualities at both ends. If it doesn't look like this, something is wrong.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E08-DADA2.sh<\/span><span>\". There is no need to supply any arguments, as the sequences are all the same length now and so do not require trimming as with the PROKs. For your interest, we're running the \"<\/span><span style = \"font-weight:bold;\">qiime dada2 denoise-single<\/span><span>\" command here.<\/span><\/li><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#EA94FF","critical":null,"critical_id":null,"duration":0},{"id":682385,"guid":"69528B80E6D511E8804139BD74755675","previous_id":682384,"previous_guid":"F3113800E6D311E8804139BD74755675","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"9B9978B42B884F9F959799FD4A4CCD3B","order_id":1,"type_id":6,"title":"Section","source":{"title":"Export the results"}},{"id":1054724,"guid":"054B89BD188D47748DBDA109CC763DB4","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">Now, let's take a look at the results, paying close attention as above to the position of the eASVs compared to the in-silico mocks.<\/div><div class = \"text-block\"><ol style = \"list-style-type: decimal;\"><li style = \"counter-reset:ol0;\"><span>Run \"<\/span><span style = \"font-weight:bold;\">E09-export-DADA2-results.sh<\/span><span>\" to export as before.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Take a look at your <\/span><span style = \"font-weight:bold;\">stats.tsv<\/span><span> first to make sure most sequences make it through to the end. We've been getting a healthy ~80% so far. This is much better than deblur, which only retains ~10% unless you cut off most of the R2.<\/span><\/li><li style = \"counter-reset:ol0;\"><span>Now, check the eASV table as before. You could have two potential issues here if the rows from the real mocks don't match up with the in-silico mocks. One is that you have spurious eASVs as above. The other is that DADA2 might truncate the reads in the middle where the tail of the R2 is. This can result in basically a R1-only eASV. Both can be identified as described above in the PROK protocol except using the \"<\/span><span style = \"font-weight:bold;\">18s-mock-blast.sh<\/span><span> \" script instead of the 16s version of that script (it's the same script, just blasting different databases).<\/span><\/li><\/ol><\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#A492FF","critical":null,"critical_id":null,"duration":0},{"id":684910,"guid":"C20BC330EDE311E8936C25CEF046BDB7","previous_id":682385,"previous_guid":"69528B80E6D511E8804139BD74755675","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"77DC80C7126944248F6D17E1CA5D819B","order_id":1,"type_id":6,"title":"Section","source":{"title":"Create barplots, cluster, export, etc"}},{"id":1054724,"guid":"E049D105E10C435288D7533BD016AF6B","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\">The remaining steps are essentially the same as for the PROK pipeline, so just follow the numbering in the scripts to guide you through the rest!<\/div><\/div>"}}],"cases":[],"data":null,"section":null,"section_color":"#94EBFF","critical":null,"critical_id":null,"duration":0}],"document":null,"materials":[],"description":"<div class = \"text-blocks\"><div class = \"text-block\">Modern sequence analysis methods allow marker gene sequences to be resolved down to single nucleotide variants variously known as Oligotypes, Exact Sequence Variants (ESVs), or Amplicon Sequence Variants (ASVs). Although the naming conventions and algorithms may differ, they all share a common goal of deriving the true biological sequence from noisy data. In this protocol, a general pipeline for determining eASVs (exact amplicon sequence variants) is presented that is suitable for amplicon datasets containing mixed 16S\/18S sequences. We have chosen to use the open-source qiime2 framework for this analysis and have added one step at the beginning to split our mixed 16S\/18S amplicon libraries using the bbsplit.sh script from the versatile BBtools suite. In our experience, both deblur and DADA2 gave similar results when benchmarked against known sequences in mock communities. Both were able to resolve amplicon sequences from mock communities down to the exact (known) sequences, although DADA2 did produce spurious eASVs under certain circumstances. These spurious eASVs were, however, confined to situations where we knew that sequence data was generated in a non-standard way. Although deblur did not suffer from this same bias, its major disadvantage for our use case was that it discarded too much data (especially for our 18S sequences where up to 90% of sequences would be discarded vs only ~20% for DADA2). Therefore, we chose to use DADA2 with careful validation of the output as described in the protocol as our default pipeline.<\/div><\/div>","changed_on":1570656843}