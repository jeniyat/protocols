{"id":23165,"title":"Enhanced convolutional neural network for plankton identification and enumeration","title_html":"Enhanced convolutional neural network for plankton identification and enumeration","image":{"source":"https:\/\/s3.amazonaws.com\/pr-journal\/bhd76xte.png","placeholder":"https:\/\/s3.amazonaws.com\/pr-journal\/bhd76xte.png"},"doi":"dx.doi.org\/10.17504\/protocols.io.2u5gey6","doi_status":2,"uri":"enhanced-convolutional-neural-network-for-plankton-2u5gey6","type_id":1,"published_on":1562861162,"parent_protocols":[],"parent_collections":[],"version_id":0,"created_on":1557802293,"categories":null,"creator":{"name":"Kaichang K. CHENG","affiliation":"Tsinghua University","affiliations":[{"affiliation":"Tsinghua University","url":"","is_default":1}],"username":"kaichang-cheng","link":"https:\/\/doi.org\/10.1371\/journal.pone.0219570","image":{"source":"https:\/\/s3.amazonaws.com\/pr-journal\/bg3p6xte.jpg","placeholder":"https:\/\/s3.amazonaws.com\/pr-journal\/bg3p6xte.jpg"},"badges":[{"id":2,"image":{"source":"\/img\/badges\/bronze.svg","placeholder":"\/img\/badges\/bronze.svg"},"name":"Author"}],"research_interests":null,"blocked_by_you":false,"blocked_you":false},"journal":"PLOS One","journal_name":"PLOS One","journal_link":"https:\/\/doi.org\/10.1371\/journal.pone.0219570","article_citation":"Cheng K,  Cheng X,  Wang Y,  Bi H,  Benfield MC (2019) Enhanced convolutional neural network for plankton identification and enumeration. PLoS ONE  14(7): e0219570. doi: <a target=\"_blank\" href=\"https:\/\/dx.doi.org\/10.1371\/journal.pone.0219570\">10.1371\/journal.pone.0219570<\/a> ","public":1,"has_versions":0,"link":"https:\/\/doi.org\/10.1371\/journal.pone.0219570","total_collections":0,"number_of_steps":6,"authors":[{"name":"Kaichang CHENG","affiliation":"Tsinghua University","affiliations":[],"username":"kaichang-cheng","link":null,"image":{"source":"https:\/\/s3.amazonaws.com\/pr-journal\/bg3p6xte.jpg","placeholder":"https:\/\/s3.amazonaws.com\/pr-journal\/bg3p6xte.jpg"},"badges":[],"research_interests":null,"blocked_by_you":false,"blocked_you":false},{"name":"Xuemin Cheng","affiliation":"Tsinghua University","affiliations":[],"username":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"research_interests":null,"blocked_by_you":false,"blocked_you":false},{"name":"Yuqi Wang","affiliation":"Tsinghua University","affiliations":[],"username":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"research_interests":null,"blocked_by_you":false,"blocked_you":false},{"name":"Hongsheng Bi","affiliation":"University of Maryland,  Center for Environmental Science, Solomons ","affiliations":[],"username":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"research_interests":null,"blocked_by_you":false,"blocked_you":false},{"name":"Mark C. Benfield","affiliation":"Louisiana State University","affiliations":[],"username":null,"link":null,"image":{"source":null,"placeholder":null},"badges":[],"research_interests":null,"blocked_by_you":false,"blocked_you":false}],"versions":[],"groups":[],"has_subprotocols":0,"is_subprotocol":0,"is_bookmarked":0,"can_be_copied":1,"can_remove_fork":1,"forks_count":{"private":0,"public":0},"access":{"can_view":1,"can_remove":0,"can_add":0,"can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":1,"can_move":1,"can_move_outside":1,"can_transfer":1,"can_download":1,"is_locked":0},"steps":[{"id":741271,"guid":"C57D762075F211E9845361430BE9CDD8","previous_id":null,"previous_guid":null,"modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"5D108A19D4244A1EB72EAAD97F1E4BEA","order_id":1,"type_id":6,"title":"Section","source":{"title":"The whole procedure"}},{"id":1054724,"guid":"A1B50C2A497744279895BBEBF990E0AD","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">The  whole procedure for the proposed method.<\/span><\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg3k6xte.jpg\" \/><\/div><div class = \"text-block\">This is a  flow chart illustrating the different steps and modules in the proposed automated plankton identification and enumeration procedure.<\/div><\/div>"}},{"id":1054725,"guid":"69D213AEF0B846DFB1580157DC9C2A95","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg3k6xte.jpg\" \/><\/div>"}}],"cases":null,"data":null,"section":null,"section_color":null,"critical":null,"critical_id":null,"duration":0},{"id":742461,"guid":"6D65FB30778011E9A87AAD02655B8ABA","previous_id":741271,"previous_guid":"C57D762075F211E9845361430BE9CDD8","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"5B2856B7E271447CA29F1F14A8DBCCA3","order_id":1,"type_id":6,"title":"Section","source":{"title":"Substeps in this procedure"}},{"id":1054724,"guid":"9E8C18BDB8884E3CAEE700BCC0C64124","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">The several substeps for the proposed method.<\/span><\/div><\/div>"}}],"cases":null,"data":null,"section":null,"section_color":null,"critical":null,"critical_id":null,"duration":0},{"id":742462,"guid":"CC761240778011E9A87AAD02655B8ABA","previous_id":742461,"previous_guid":"6D65FB30778011E9A87AAD02655B8ABA","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"94EF93931C7B42E981C105276EC071F8","order_id":1,"type_id":6,"title":"Section","source":{"title":"Substeps in this procedure"}},{"id":1054724,"guid":"AFA022E94BC2441685789B16A850F551","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">Adaptive ROI extraction based on the following steps.<\/span><\/div><div class = \"text-block\">Calculate the mean value of image intensity based on the following formula:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7r6xte.jpg\" \/><\/div><div class = \"text-block\">Calculate the Mean Signal-to-Noise Ratio  value of the image  based on the following formula:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7s6xte.jpg\" \/><\/div><div class = \"text-block\">Calculate the threshold of the image based on Sauvola's method:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7t6xte.jpg\" \/><\/div><div class = \"text-block\">Binarize the image based on Sauvola's method.<\/div><div class = \"text-block\">Extract ROIs based on the connected domain of the binarized image.<\/div><\/div>"}},{"id":1054725,"guid":"E649177E72364C638475BF24863539F7","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7r6xte.jpg\" \/><\/div>"}},{"id":1054726,"guid":"5008ECCE290640CA8C0606C6DADD5A1A","order_id":3,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7s6xte.jpg\" \/><\/div>"}},{"id":1054727,"guid":"472CF2922FBA4FAF86A1144EA71E7DDB","order_id":4,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7t6xte.jpg\" \/><\/div>"}}],"cases":null,"data":null,"section":null,"section_color":null,"critical":null,"critical_id":null,"duration":0},{"id":742463,"guid":"AACE63D0778111E9A87AAD02655B8ABA","previous_id":742462,"previous_guid":"CC761240778011E9A87AAD02655B8ABA","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"170D1561973F48D6987294CDFBCBB76B","order_id":1,"type_id":6,"title":"Section","source":{"title":"Substeps in this procedure"}},{"id":1054724,"guid":"52D12E945A9946C09A28D410D8FEC444","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">ROI enhancement based on the following method.<\/span><\/div><div class = \"text-block\">A. Target feature enhancement based on the following formula:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7u6xte.jpg\" \/><\/div><div class = \"text-block\">B. Background suppression based on the following formula:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7v6xte.jpg\" \/><\/div><\/div>"}},{"id":1054725,"guid":"F6A3E400B7A146FE8EC9234A80D8EC22","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7u6xte.jpg\" \/><\/div>"}},{"id":1054726,"guid":"1F37E638495A466E8C249CE1C4EC3426","order_id":3,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7v6xte.jpg\" \/><\/div>"}}],"cases":null,"data":null,"section":null,"section_color":null,"critical":null,"critical_id":null,"duration":0},{"id":742464,"guid":"EEA666C0778111E9A87AAD02655B8ABA","previous_id":742463,"previous_guid":"AACE63D0778111E9A87AAD02655B8ABA","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"A806B82DDC3C4212BB06DFE4C06803A7","order_id":1,"type_id":6,"title":"Section","source":{"title":"Substeps in this procedure"}},{"id":1054724,"guid":"0C8140D15C3B484E8B248DA08A869D83","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">Train CNN combined with multi-class SVM model based on the in situ plankton databset.<\/span><\/div><div class = \"text-block\">The fully connected layers from the selected CNN models were used to describe sample features and were used as the input for the multi-class SVM model.  <\/div><div class = \"text-block\">The multi-class SVM model is a one vs. one type,  and the linear relationship between the input and output can be described by the following formula:<\/div><div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7w6xte.jpg\" \/><\/div><div class = \"text-block\">Save the trained model for next identification and enumeration.<\/div><div class = \"text-block\">Note: you can get the dataset  at <\/div><div class = \"text-block\"><a href=\"https:\/\/doi.org\/10.6084\/m9.figshare.8146283\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/doi.org\/10.6084\/m9.figshare.8146283<\/span><\/a><\/div><div class = \"text-block\">.<\/div><\/div>"}},{"id":1054725,"guid":"A44697CE8EE142269D464B1F1451360E","order_id":2,"type_id":1,"title":"description","source":{"description":"<div style = \"text-align :; float : ;\"><img style = \"\" src = \"https:\/\/s3.amazonaws.com\/pr-journal\/bg7w6xte.jpg\" \/><\/div>"}}],"cases":null,"data":null,"section":null,"section_color":null,"critical":null,"critical_id":null,"duration":0},{"id":742465,"guid":"6B0E6F40778311E9A87AAD02655B8ABA","previous_id":742464,"previous_guid":"EEA666C0778111E9A87AAD02655B8ABA","modified_on":0,"protocol_id":0,"components":[{"id":1054723,"guid":"2C73961B605B453399BF74EA84AF8383","order_id":1,"type_id":6,"title":"Section","source":{"title":"Substeps in this procedure"}},{"id":1054724,"guid":"CCE3EDB2904741CA98FC600E3E86AB71","order_id":1,"type_id":1,"title":"description","source":{"description":"<div class = \"text-blocks\"><div class = \"text-block\"><span style = \"font-weight:bold;\">Identification and enumeration.<\/span><\/div><div class = \"text-block\">The input of this procedure is the original in situ plankton image, and the output (statistical results of every plankton image for each class) is save in a file in the '. xlsx' format.<\/div><div class = \"text-block\">Note: you can learn more details in our open codes in MATLAB language at <\/div><div class = \"text-block\"><a href=\"https:\/\/github.com\/KaichangCHENG\/PIE-MC\/tree\/master\/EnhancedCNN\" style = \"text-decoration:underline;color:blue;cursor:pointer;\"><span style = \":;\">https:\/\/github.com\/KaichangCHENG\/PIE-MC\/tree\/master\/EnhancedCNN<\/span><\/a><\/div><div class = \"text-block\">.<\/div><\/div>"}}],"cases":null,"data":null,"section":null,"section_color":null,"critical":null,"critical_id":null,"duration":0}],"materials":[],"description":"<div class = \"text-blocks\"><div class = \"text-block\">This is an automatic plankton image recognition and enumeration system using an enhanced Convolutional Neural Network (CNN) and examined the performance of different network structures on automatic plankton image classification.<\/div><\/div>","changed_on":1562861162}