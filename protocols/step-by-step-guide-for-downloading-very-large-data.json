{"uri":"step-by-step-guide-for-downloading-very-large-data-kb6csre","version_id":"0","protocol_name":"Step-by-Step guide for downloading very large datasets to a supercomputer using the SRA Toolkit \n","protocol_name_html":"Step-by-Step guide for downloading very large datasets to a supercomputer using the SRA Toolkit \n","is_prepublished":"0","can_edit":"0","parent_id":null,"api_version":"1","is_new_mode":"0","last_modified":"1522044002","type_id":"1","link":null,"fork_id":"","public_fork_note":"","number_of_steps":"7","has_versions":"0","first_published_date":"1508265126","publish_date":"2017-10-17 18:32:06","documents":null,"have_protocol_in_step":"0","is_protocol_in_step":"0","vendor_name":"Contributed by users","vendor_link":"https:\/\/www.protocols.io","vendor_logo":"\/img\/vendors\/1.png","mod_mins":"-45","mod_secs":"1","description":null,"is_bookmarked":"0","can_reassign":"1","before_start":null,"has_guidelines":"0","materials":[],"warning":null,"version_class":"8286","public":"1","is_owner":"1","is_original_owner":"1","created_on":"1508262029","protocol_affiliation":"National Center for Supercomputing Applications, University of Illinois at Urbana-champaign,Department of Health Sciences Research, Mayo Clinic, Jacksonville, FL","affiliation":null,"doi":"dx.doi.org\/10.17504\/protocols.io.kb6csre","doi_status":"2","changed_fork_steps":null,"profile_url":"y2u2a4v2x2","protocol_img":"https:\/\/www.protocols.io\/img\/default_protocol.png","profile_image":"\/img\/avatars\/012.png","full_name":"Katherine Kendig","created_by":"Katherine Kendig","private_link":"9AA7468C6E7B2AAB4D7AC5077948CCD9","original_img":"1","username":"katherine-kendig","is_retracted":"0","retraction_reason":null,"plos_id":null,"manuscript_citation":null,"journal_name":null,"is_donations_disabled":"0","is_donations_disabled_by_user":"9","item_record_id":268925,"fork_info":[],"compare_forks":[],"protocols":[],"groups":[],"number_of_shared_runs":[],"ownership_history":[{"created_on":"2017-10-17 17:40:29","change_time":"2017-10-17 17:40:29","username":"liudmila-sergeevna-mainzer","full_name":"Liudmila Sergeevna Mainzer","profile_image":"\/img\/avatars\/004.png","affiliation":null},{"created_on":"2018-01-26 16:49:38","change_time":"2018-01-26 16:49:38","username":"katherine-kendig","profile_image":"\/img\/avatars\/012.png","full_name":"Katherine Kendig","affiliation":null}],"keywords":"","transfer_to_user":[],"sub_transfer":false,"is_transfer_pending":false,"number_of_bookmarks":"1","collections":[],"tags":[],"archived":0,"sub_authors":[],"sub_protocols_number":0,"can_edit_shared":0,"shared_runs":[],"is_shared_run":0,"is_shared":1,"banner":null,"contact_badges":[{"badge_id":"2","badge_image":"\/img\/badges\/bronze.svg","badge_description":"Author!"},{"badge_id":"5","badge_image":"\/img\/badges\/earlyadopter.svg","badge_description":"Early adopter"}],"number_of_comments":0,"is_locked":0,"is_locked_by":false,"authors":"Jacob Heldenbrand,Yingxue Ren,Yan Asmann,Liudmila S. Mainzer","authors_list":[{"name":"Jacob Heldenbrand","affiliation":"National Center for Supercomputing Applications, University of Illinois at Urbana-champaign","username":null,"profile_image":null},{"name":"Yingxue Ren","affiliation":"Department of Health Sciences Research, Mayo Clinic, Jacksonville, FL","username":null,"profile_image":null},{"name":"Yan Asmann","affiliation":"Department of Health Sciences Research, Mayo Clinic, Jacksonville, FL","username":null,"profile_image":null},{"name":"Liudmila S. Mainzer","affiliation":"National Center for Supercomputing Applications, University of Illinois at Urbana-champaign","username":null,"profile_image":null}],"user":{"profile_image":"\/img\/avatars\/012.png","username":"katherine-kendig","full_name":"Katherine Kendig","created_by":"Katherine Kendig"},"access":{"can_view":"1","can_remove":"0","can_add":"0","can_edit":0,"can_publish":0,"can_get_doi":0,"can_share":"0","can_move":"1","can_transfer":"1","can_download":"1","is_locked":"0"},"is_contact_suspended":0,"guidelines":null,"status_id":"1","is_research":"1","status_info":null,"steps":[{"id":"591097","is_changed":0,"original_id":"0","is_skipped":"0","is_checked":"0","guid":"F257EE0ED47347658B70153D65569FC1","previous_guid":null,"previous_id":"0","last_modified":"1508262104","components":[{"component_id":"1013115","previous_id":0,"original_id":"0","guid":"2E93DC159DF949329BEFD4A5A97ED78E","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Introduction","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Introduction"},"is_project":0},{"component_id":"1013114","previous_id":"1013115","original_id":"0","guid":"96C7F85EC7FD420C8ED7DFC086C215D9","previous_guid":"2E93DC159DF949329BEFD4A5A97ED78E","component_type_id":"1","data_id":null,"data":"<p>The SRA Toolkit is a complex piece of software that can be difficult to navigate, as the documentation is extensive and error messages are not always able to provide clarity when failures occur. In an effort to maximize the Toolkit\u2019s utility, we have devised a protocol for downloading thousands of SRA files and converting them into FASTQ files in a reasonable amount of time. To make the process as simple as possible, our protocol anticipates and adjusts for likely errors. While this guide has a limited lifespan in the face of regular updates to the Toolkit, we hope that our instructions will be of help to the community, as they summarize a lot of disjoint information already floating around on the Web.<\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>The SRA Toolkit is a complex piece of software that can be difficult to navigate, as the documentation is extensive and error messages are not always able to provide clarity when failures occur. In an effort to maximize the Toolkit\u2019s utility, we have devised a protocol for downloading thousands of SRA files and converting them into FASTQ files in a reasonable amount of time. To make the process as simple as possible, our protocol anticipates and adjusts for likely errors. While this guide has a limited lifespan in the face of regular updates to the Toolkit, we hope that our instructions will be of help to the community, as they summarize a lot of disjoint information already floating around on the Web.<\/p>"},"is_project":0}],"available_protocols":[]},{"id":"591098","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"4E842B7E96654EB795392555F1772C17","previous_guid":"F257EE0ED47347658B70153D65569FC1","previous_id":"591097","last_modified":"1508262150","components":[{"component_id":"1013117","previous_id":0,"original_id":"0","guid":"D8B731BFD984427688E3B90BC463A086","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Tools","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Tools"},"is_project":0},{"component_id":"1013116","previous_id":"1013117","original_id":"0","guid":"544A9C201A1249FC8BE6DAD1EC3DA234","previous_guid":"D8B731BFD984427688E3B90BC463A086","component_type_id":"1","data_id":null,"data":"<ul>\n<li>prefetch\u2014For downloading the SRA files themselves from NCBI<\/li>\n<li>vdb-config\u2014Must use this to configure the toolkit and specify the location of the dbGaP private key<\/li>\n<li>sra-validate\u2014Tool that performs a checksum on SRA to ensure transfer of data was successful<\/li>\n<li>fastq-dump\u2014For converting the SRA files into the FASTQ format for easy use<\/li>\n<li><a href=\"https:\/\/github.com\/ncsa\/Scheduler\" target=\"_blank\">Anisimov Launcher<\/a>\u2014Blue Waters tool that launches multiple jobs in parallel<\/li>\n<li>Aspera\u2014Download tool<\/li>\n<\/ul>\n<p>\u00a0<\/p>\n<p><em>Note1: This protocol assumes you are downloading dbGaP data. If not, skip the private key configuration steps.<\/em><\/p>\n<p><em>Note2: We designed this Guide for downloads to Blue Waters. With small adjustments, it should be applicable to other clusters. <\/em><\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<ul>\n<li>prefetch\u2014For downloading the SRA files themselves from NCBI<\/li>\n<li>vdb-config\u2014Must use this to configure the toolkit and specify the location of the dbGaP private key<\/li>\n<li>sra-validate\u2014Tool that performs a checksum on SRA to ensure transfer of data was successful<\/li>\n<li>fastq-dump\u2014For converting the SRA files into the FASTQ format for easy use<\/li>\n<li><a href=\"https:\/\/github.com\/ncsa\/Scheduler\" target=\"_blank\">Anisimov Launcher<\/a>\u2014Blue Waters tool that launches multiple jobs in parallel<\/li>\n<li>Aspera\u2014Download tool<\/li>\n<\/ul>\n<p>\u00a0<\/p>\n<p><em>Note1: This protocol assumes you are downloading dbGaP data. If not, skip the private key configuration steps.<\/em><\/p>\n<p><em>Note2: We designed this Guide for downloads to Blue Waters. With small adjustments, it should be applicable to other clusters. <\/em><\/p>"},"is_project":0}]},{"id":"591099","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"77950B92F4094284A772C366121D0DCF","previous_guid":"4E842B7E96654EB795392555F1772C17","previous_id":"591098","last_modified":"1508262303","components":[{"component_id":"1013119","previous_id":0,"original_id":"0","guid":"7BDFF8555ADC4763A5FB4C0D825F6DEF","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Assumed File Structure","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Assumed File Structure"},"is_project":0},{"component_id":"1013118","previous_id":"1013119","original_id":"0","guid":"99CC894979D84B29B6ED2624C444C12E","previous_guid":"7BDFF8555ADC4763A5FB4C0D825F6DEF","component_type_id":"1","data_id":null,"data":"<p>The explanations and scripts below assume the following file structure. If it is modified, the scripts must be altered as well.<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/pg7hzyw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/pg7hzyw.png\" data-ofn=\"FileStructure.png\" \/><\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>The explanations and scripts below assume the following file structure. If it is modified, the scripts must be altered as well.<\/p>\n<p><img id=\"s-mce-img\" class=\"s-mce-img\" src=\"https:\/\/s3.amazonaws.com\/pr-journal\/pg7hzyw.png\" data-src=\"https:\/\/s3.amazonaws.com\/pr-journal\/pg7hzyw.png\" data-ofn=\"FileStructure.png\" \/><\/p>"},"is_project":0}]},{"id":"591100","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"5D2BD599B2F84B31A9265DE5D5B83901","previous_guid":"77950B92F4094284A772C366121D0DCF","previous_id":"591099","last_modified":"1508262757","components":[{"component_id":"1013121","previous_id":0,"original_id":"0","guid":"EFB344BBD6A14BB59EEF0BF30BDF8DFD","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Preparation","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Preparation"},"is_project":0},{"component_id":"1013120","previous_id":"1013121","original_id":"0","guid":"427B90BE936346BDB61A46240B63FD83","previous_guid":"EFB344BBD6A14BB59EEF0BF30BDF8DFD","component_type_id":"1","data_id":null,"data":"<p>The download procedure is normally a two-step process: first grab the SRA files from the repository, then convert SRA files to FASTQ on the cluster. A few preparatory steps will help avoid bottlenecks.<\/p>\n<h2 style=\"margin-bottom: 6.0pt;\"><a target=\"_blank\">vdb-config<\/a><\/h2>\n<p>Run the following command to execute vdb-config (located within the SRA toolkit bin folder). This may require X11 forwarding (ssh \u2013X flag on login to cluster).<\/p>\n<pre class=\"language-markup\"><code>.\/vdb-config \u2013i<\/code><\/pre>\n<p>This opens a GUI where the location of the dbGaP project space can be configured. Set this to Project_Space. If downloading dbGaP data, specify the repository key location.<\/p>\n<p>\u00a0<\/p>\n<p>Make sure this is done before downloading the refseq data below, as the tool will not allow you to point to a directory in which the sra and refseq subdirectories are not empty.<\/p>\n<h2><a target=\"_blank\"><\/a>refseq download<\/h2>\n<p>To convert an SRA file to the FASTQ format, fastq-dump must normally download reference data stored in a refseq database at NCBI. However, this creates a bottleneck when trying to scale up conversions of many files, as the reference data end up being downloaded repeatedly for every file batch.<\/p>\n<p>\u00a0<\/p>\n<p>To circumvent this bottleneck, we manually downloaded all the reference files located at <a href=\"https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/\" target=\"_blank\">https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/<\/a>. While this is a large download of ~40GB, it only needs to be done once. Furthermore, the SRA Toolkit is configured to download any missing reference files if it cannot find them later during the SRA to FASTQ conversion stage. Thus, if new reference files are added to the repository between your bulk reference download and the actual data conversion, you should still get correct results when running fastq-dump.<\/p>\n<p>\u00a0<\/p>\n<p>To download the reference files, copy the contents of <a href=\"https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/\" target=\"_blank\">https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/<\/a> into an Excel sheet, grab the names of each file, and put them in a file named \/base\/refseq_download\/list_all_refseqs.txt.<\/p>\n<p>\u00a0<\/p>\n<p>Use the following bash script as a wrapper to call the python script that downloads the reference files. Wrap the bash script in a qsub and submit it to a compute node. Using ten download processes in parallel by breaking up \/base\/refseq_download\/list_all_refseqs.txt\u00a0 into 10 batches will increase efficiency.<\/p>\n<pre class=\"language-markup\"><code>\/base\/refseq_download\/download_parallel_wrapper.sh\n\n#!\/bin\/bash\n\npython base\/refseq_download\/download_refseqs_parallel.py 0 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 1 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 2 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 3 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 4 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 5 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 6 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 7 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 8 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 9 &amp;\nwait\n<\/code><\/pre>\n<pre class=\"language-markup\"><code>\/base\/refseq_download\/download_refseqs_parallel.py\n\nimport sys\nimport subprocess\n\nN = int(sys.argv[1])\n\nfilenames = []\n\nwith open('\/base\/refseq_download\/list_all_refseqs.txt') as F:\n        for line in F:\n                name = line.strip()\n                filenames.append(name)\n\n# Start at position N and go to the end in 10 step intervals\nfor i in filenames[N::10]:\n        subprocess.check_call(\"~\/.aspera\/connect\/bin\/ascp \\\n        -i ~\/.aspera\/connect\/etc\/asperaweb_id_dsa.openssh -k 1 -T -l800m \\    \n        anonftp@ftp.ncbi.nlm.nih.gov:\/sra\/refseq\/{0} \\\n        \/base\/Project_Space\/refseq\/\".format(i), shell=True)\n<\/code><\/pre>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>The download procedure is normally a two-step process: first grab the SRA files from the repository, then convert SRA files to FASTQ on the cluster. A few preparatory steps will help avoid bottlenecks.<\/p>\n<h2 style=\"margin-bottom: 6.0pt;\"><a target=\"_blank\">vdb-config<\/a><\/h2>\n<p>Run the following command to execute vdb-config (located within the SRA toolkit bin folder). This may require X11 forwarding (ssh \u2013X flag on login to cluster).<\/p>\n<pre class=\"language-markup\"><code>.\/vdb-config \u2013i<\/code><\/pre>\n<p>This opens a GUI where the location of the dbGaP project space can be configured. Set this to Project_Space. If downloading dbGaP data, specify the repository key location.<\/p>\n<p>\u00a0<\/p>\n<p>Make sure this is done before downloading the refseq data below, as the tool will not allow you to point to a directory in which the sra and refseq subdirectories are not empty.<\/p>\n<h2><a target=\"_blank\"><\/a>refseq download<\/h2>\n<p>To convert an SRA file to the FASTQ format, fastq-dump must normally download reference data stored in a refseq database at NCBI. However, this creates a bottleneck when trying to scale up conversions of many files, as the reference data end up being downloaded repeatedly for every file batch.<\/p>\n<p>\u00a0<\/p>\n<p>To circumvent this bottleneck, we manually downloaded all the reference files located at <a href=\"https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/\" target=\"_blank\">https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/<\/a>. While this is a large download of ~40GB, it only needs to be done once. Furthermore, the SRA Toolkit is configured to download any missing reference files if it cannot find them later during the SRA to FASTQ conversion stage. Thus, if new reference files are added to the repository between your bulk reference download and the actual data conversion, you should still get correct results when running fastq-dump.<\/p>\n<p>\u00a0<\/p>\n<p>To download the reference files, copy the contents of <a href=\"https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/\" target=\"_blank\">https:\/\/ftp.ncbi.nlm.nih.gov\/sra\/refseq\/<\/a> into an Excel sheet, grab the names of each file, and put them in a file named \/base\/refseq_download\/list_all_refseqs.txt.<\/p>\n<p>\u00a0<\/p>\n<p>Use the following bash script as a wrapper to call the python script that downloads the reference files. Wrap the bash script in a qsub and submit it to a compute node. Using ten download processes in parallel by breaking up \/base\/refseq_download\/list_all_refseqs.txt\u00a0 into 10 batches will increase efficiency.<\/p>\n<pre class=\"language-markup\"><code>\/base\/refseq_download\/download_parallel_wrapper.sh\n\n#!\/bin\/bash\n\npython base\/refseq_download\/download_refseqs_parallel.py 0 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 1 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 2 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 3 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 4 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 5 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 6 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 7 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 8 &amp;\npython base\/refseq_download\/download_refseqs_parallel.py 9 &amp;\nwait\n<\/code><\/pre>\n<pre class=\"language-markup\"><code>\/base\/refseq_download\/download_refseqs_parallel.py\n\nimport sys\nimport subprocess\n\nN = int(sys.argv[1])\n\nfilenames = []\n\nwith open('\/base\/refseq_download\/list_all_refseqs.txt') as F:\n        for line in F:\n                name = line.strip()\n                filenames.append(name)\n\n# Start at position N and go to the end in 10 step intervals\nfor i in filenames[N::10]:\n        subprocess.check_call(\"~\/.aspera\/connect\/bin\/ascp \\\n        -i ~\/.aspera\/connect\/etc\/asperaweb_id_dsa.openssh -k 1 -T -l800m \\    \n        anonftp@ftp.ncbi.nlm.nih.gov:\/sra\/refseq\/{0} \\\n        \/base\/Project_Space\/refseq\/\".format(i), shell=True)\n<\/code><\/pre>"},"is_project":0}]},{"id":"591101","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"B964533356A34C5A896443562DB42400","previous_guid":"5D2BD599B2F84B31A9265DE5D5B83901","previous_id":"591100","last_modified":"1508263341","components":[{"component_id":"1013123","previous_id":0,"original_id":"0","guid":"E4379AFBB1A348819530103DD3B0BA71","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Data Download: SRA files","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Data Download: SRA files"},"is_project":0},{"component_id":"1013122","previous_id":"1013123","original_id":"0","guid":"EA0E5CC8AB0045C5A9A58929FD3EFB4A","previous_guid":"E4379AFBB1A348819530103DD3B0BA71","component_type_id":"1","data_id":null,"data":"<h2><a target=\"_blank\"><\/a>Step 1: Create Batch List<\/h2>\n<p>For each batch, create a text file in \/base\/batches\/batch_lists\/. For this protocol, we will refer to the current batch being downloaded as batchX. Therefore, create a file like the following:<\/p>\n<pre class=\"language-markup\"><code>\/base\/batches\/batch_list\/batchX.txt\n\nSRR123\nSRR234\nSRR345\nSRR456\nSRR567\n<\/code><\/pre>\n<p>... and so on.<\/p>\n<p><em>Note: Using this procedure we downloaded ~ 10,000 SRA files broken up into batches of 1,200 SRAs.<\/em><\/p>\n<h2><a target=\"_blank\"><\/a>Step 2: Create JobList and prefetch bash scripts for the Anisimov Launcher<\/h2>\n<p>This step is designed to bundle individual single-threaded download tasks into an MPI job that can run across multiple nodes. This increases queue priority, and facilitates efficient use of nodes on clusters that espouse node exclusivity (no more than one user per node). The launcher is a simple MPI wrapper, which takes in a list of all the individual tasks (JobList.txt) and places them on the available cores within the multi-node qsub reservation on the cluster. If you give it more tasks than cores, then it will start the first batch of tasks on the available cores, and keep starting new ones as the tasks complete and cores become available. For each ID in this batch, use the generateBatchScripts.prefetch.py script to automatically create a bash script like the following. This is your \u201cAnisimov task\u201d for this batch:<\/p>\n<pre class=\"language-markup\"><code>#!\/bin\/bash\n\n# If downloading dbGaP data, prefetch must be called from within the project space folder\ncd \/base\/Project_Space\n\n# Download the SRA file\n\/path\/to\/sra-toolkit\/bin\/prefetch -L debug -t fasp -v -v &lt;SAMPLE_ID&gt;\n<\/code><\/pre>\n<p>The generateBatchScripts.prefetch.py will also construct the JobList.txt file that lists the names and locations of these scripts in the following format:<\/p>\n<pre class=\"language-markup\"><code>\/base\/batches\/batchX\/SRR123 SRR123.sh\n\/base\/batches\/batchX\/SRR234 SRR234.sh\n\/base\/batches\/batchX\/SRR345 SRR345.sh\n\/base\/batches\/batchX\/SRR456 SRR456.sh\n\/base\/batches\/batchX\/SRR567 SRR567.sh<\/code><\/pre>\n<p>... and so on.<\/p>\n<p>Use this Python script to generate both the shell scripts for each sample and the jobList file:<\/p>\n<pre class=\"language-markup\"><code>\/base\/scripts\/generateBatchScripts.prefetch.py (modify with paths to suit your needs)\n\n#!\/usr\/bin\/python\n\nimport os\nimport os.path\nimport sys\n\n### GLOBAL VARIABLES\n\nsraListFile = sys.argv[1]\nbatchName = sys.argv[2]\n\nbatchFullPath = \"base\/batches\/\" + batchName\n\nSRA_list = []\n\n### FUNCTION DEFINITIONS\n\ndef createScripts(SRA_ID):\n    subDirName = batchFullPath + \"\/\" + SRA_ID\n\n    # Create the subdirectory within the batch directory\n    if (not os.path.isdir(subDirName)):\n        os.mkdir(subDirName)\n    \n    # Create the shell script file\n    shellFile = open(subDirName + \"\/\" + SRA_ID + \".sh\", \"w\")\n\n    # Write to the file\n    shellFile.write(\"#!\/bin\/bash\\n\\n\")\n\n    shellFile.write(\"cd base\/Project_Space\\n\\n\")\n\n    shellFile.write(\"# Download the SRA file\\n\")\n    shellFile.write(\"\/path\/to\/sra-toolkit\/bin\/prefetch -L debug -t fasp -v -v \" \\\n                     + SRA_ID + \"\\n\\n\")\n    shellFile.close()\n\ndef makeJobListFile():\n    jobListFile = open(\"base\/jobLists\/\" + batchName + \"_JobList.txt\", \"w\")\n    for i in SRA_list:\n        # Write the jobList for the Anisimov launcher\n        # Something like \"\/base\/batches\/batch1\/SRR123 SRR123.sh\"\n        jobListFile.write(batchFullPath + \"\/\" + i + \" \" + i + \".sh\\n\")\n    jobListFile.close()\n\n### IMPLEMENTATION\n\n# Get the list of SRA IDs\nwith open(sraListFile) as F:\n    for line in F:\n\tSRA_list.append(line.strip())\n\n# If the batch directory does not exist, create it\nif (not os.path.isdir(batchFullPath)):\n    os.mkdir(batchFullPath)\n# Create the subdirectories and shell scripts\nfor i in SRA_list:\n    createScripts(i)\n\nmakeJobListFile()\n<\/code><\/pre>\n<p>There are two arguments passed to this script at runtime: the location of the batchX.txt file created earlier and the name of the batch. It can be invoked with the following command:<\/p>\n<pre class=\"language-markup\"><code>python generateBatchScripts.prefetch.py \/base\/batches\/batch_lists\/batchX.txt batchX<\/code><\/pre>\n<p>This will generate the jobList.txt file and the necessary bash scripts and put them in their own directory within the batches folder:<\/p>\n<pre class=\"language-markup\"><code>\/base\/batches\/batchX\/SRR123\/SRR123.sh\n\/base\/batches\/batchX\/SRR234\/SRR234.sh\n\/base\/batches\/batchX\/SRR345\/SRR345.sh<\/code><\/pre>\n<p>... and so on.<\/p>\n<h2><a target=\"_blank\"><\/a>Step 3: Create the prefetch qsub script<\/h2>\n<p>To use the Anisimov Launcher to schedule jobs, construct a qsub script. These are stored in the \/base\/qsubs directory. Our tests suggest that 15 samples can be downloaded on a node simultaneously, as long as they are spaced out over the cores (on a cray system we supply the \u201c-d 2\u201d flag to aprun).<\/p>\n<pre class=\"language-markup\"><code>#nodes = batch_size\/15 \nAprun\u2019s -n flag = #nodes * 16\nAprun\u2019s -N flag = 16\n<\/code><\/pre>\n<p>The qsub script should look something like the example qsub below, which assumes a batch of 1,200 samples.<\/p>\n<pre class=\"language-markup\"><code>\/base\/qsubs\/batchX_prefetch.qsub \n\n#!\/bin\/bash\n#PBS -N sra_X\n#PBS -l walltime=1:15:00\n#PBS -l nodes=80:ppn=32\n#PBS -A groupid\n#PBS -q normal\n\naprun -n 1280 -N 16 -ss -d 2 ~anisimov\/scheduler\/scheduler.x \\   \n  \/base\/jobLists\/batchX_JobList.txt \/bin\/bash -noexit &amp;&gt; \/base\/qsubs\/logs\/batchX_prefetch.log\n<\/code><\/pre>\n<p>This script will launch all the bash scripts (15\/node). Both stdout and stderr will be piped to \/base\/qsubs\/logs\/batchX_prefetch.log.<\/p>\n<h2><a target=\"_blank\"><\/a>Step 4: Running the prefetch qsub script<\/h2>\n<p>Unfortunately, as prefetch runs, some of the SRA downloads will fail. To prevent a single failure from killing the Anisimov Job and the other downloads occurring in parallel, the -noexit flag is used (see the box above). However, this means the download may eventually reach a point at which all the SRAs have finished downloading, but the job just sits without making progress. This is just a consequence of the Anisimov Launcher code design.<\/p>\n<p>\u00a0<\/p>\n<p>To prevent this from wasting resources, monitor the size of the \/base\/Project_Space\/sra folder during the download using the following command:<\/p>\n<pre class=\"language-markup\"><code>ls \u2013l \/base\/Project_Space\/sra | head<\/code><\/pre>\n<p>If the size of the sra\/ folder does not appear to grow for five minutes or so, go ahead and kill the job (yes, it is a hack at this point, and one could automate it if desired):<\/p>\n<pre class=\"language-markup\"><code>qdel JobID<\/code><\/pre>\n<p>As the SRAs are downloaded, temporary files are generated in the sra\/ folder. If those files are present the next time you attempt to download this ID, the download will fail. To prevent this from happening, delete the .tmp and .lock files with the following commands:<\/p>\n<pre class=\"language-markup\"><code>cd \/base\/Project_Space\/sra\n\nrm *.tmp.aspera-ckpt\nrm *.tmp.partial\nrm *.lock\nrm *.tmp\n\nrm *.vdbcache.cache\nrm *.vdbcache\n<\/code><\/pre>\n<h2><a target=\"_blank\"><\/a>Step 5: Determine which SRA IDs did not finish downloading<\/h2>\n<p>After removing the .tmp and .lock files, run the following script:<\/p>\n<pre class=\"language-markup\"><code>\/bash\/scripts\/checkSRAsDownloaded.py\n\nimport sys\nimport glob\n\nbatch_list = sys.argv[1]\n\n# List of IDs\nbatch_IDs = []\n\nwith open(batch_list) as F:\n    for line in F:\n        batch_IDs.append(line.strip())\n\nIDs_found = []\n\nfor f in glob.glob(\"\/base\/Project_Space\/sra\/*\"):\n    split_string = f.split(\"\/\")\n    ID = split_string[-1].split('.')[0]\n    IDs_found.append(ID)\n\n# Remove redundant\nIDs_found = list(set(IDs_found))\n\ncount_missing = 0\n\nfor i in batch_IDs:\n    if i not in IDs_found:\n        print(i)\n        count_missing += 1\n\nprint(\"\\nIDs that are missing\")\nprint(count_missing)\n<\/code><\/pre>\n<p>This script grabs the IDs in the batch file and checks to see whether each SRA file is found in the sra\/ folder. Invoke with the following command:<\/p>\n<pre class=\"language-markup\"><code>python \/base\/scripts\/checkSRAsDownloaded.py \/base\/batches\/batch_lists\/batchX.txt<\/code><\/pre>\n<p>Any SRA IDs that are not present in the sra\/ folder will be printed out, as well as the total number that were not downloaded. Copy these IDs and put them in a new batch.txt file in \/base\/batches\/batch_lists. We found it effective to name this new file batchX.1, then name the next iteration batchX.2, and so on. Repeat the downloading steps until all SRA IDs are accounted for.<\/p>\n<p>\u00a0<\/p>\n<p><em>Note: On each iteration, reduce the resources requested in each qsub script so that resources are not wasted.<\/em><\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<h2><a target=\"_blank\"><\/a>Step 1: Create Batch List<\/h2>\n<p>For each batch, create a text file in \/base\/batches\/batch_lists\/. For this protocol, we will refer to the current batch being downloaded as batchX. Therefore, create a file like the following:<\/p>\n<pre class=\"language-markup\"><code>\/base\/batches\/batch_list\/batchX.txt\n\nSRR123\nSRR234\nSRR345\nSRR456\nSRR567\n<\/code><\/pre>\n<p>... and so on.<\/p>\n<p><em>Note: Using this procedure we downloaded ~ 10,000 SRA files broken up into batches of 1,200 SRAs.<\/em><\/p>\n<h2><a target=\"_blank\"><\/a>Step 2: Create JobList and prefetch bash scripts for the Anisimov Launcher<\/h2>\n<p>This step is designed to bundle individual single-threaded download tasks into an MPI job that can run across multiple nodes. This increases queue priority, and facilitates efficient use of nodes on clusters that espouse node exclusivity (no more than one user per node). The launcher is a simple MPI wrapper, which takes in a list of all the individual tasks (JobList.txt) and places them on the available cores within the multi-node qsub reservation on the cluster. If you give it more tasks than cores, then it will start the first batch of tasks on the available cores, and keep starting new ones as the tasks complete and cores become available. For each ID in this batch, use the generateBatchScripts.prefetch.py script to automatically create a bash script like the following. This is your \u201cAnisimov task\u201d for this batch:<\/p>\n<pre class=\"language-markup\"><code>#!\/bin\/bash\n\n# If downloading dbGaP data, prefetch must be called from within the project space folder\ncd \/base\/Project_Space\n\n# Download the SRA file\n\/path\/to\/sra-toolkit\/bin\/prefetch -L debug -t fasp -v -v &lt;SAMPLE_ID&gt;\n<\/code><\/pre>\n<p>The generateBatchScripts.prefetch.py will also construct the JobList.txt file that lists the names and locations of these scripts in the following format:<\/p>\n<pre class=\"language-markup\"><code>\/base\/batches\/batchX\/SRR123 SRR123.sh\n\/base\/batches\/batchX\/SRR234 SRR234.sh\n\/base\/batches\/batchX\/SRR345 SRR345.sh\n\/base\/batches\/batchX\/SRR456 SRR456.sh\n\/base\/batches\/batchX\/SRR567 SRR567.sh<\/code><\/pre>\n<p>... and so on.<\/p>\n<p>Use this Python script to generate both the shell scripts for each sample and the jobList file:<\/p>\n<pre class=\"language-markup\"><code>\/base\/scripts\/generateBatchScripts.prefetch.py (modify with paths to suit your needs)\n\n#!\/usr\/bin\/python\n\nimport os\nimport os.path\nimport sys\n\n### GLOBAL VARIABLES\n\nsraListFile = sys.argv[1]\nbatchName = sys.argv[2]\n\nbatchFullPath = \"base\/batches\/\" + batchName\n\nSRA_list = []\n\n### FUNCTION DEFINITIONS\n\ndef createScripts(SRA_ID):\n    subDirName = batchFullPath + \"\/\" + SRA_ID\n\n    # Create the subdirectory within the batch directory\n    if (not os.path.isdir(subDirName)):\n        os.mkdir(subDirName)\n    \n    # Create the shell script file\n    shellFile = open(subDirName + \"\/\" + SRA_ID + \".sh\", \"w\")\n\n    # Write to the file\n    shellFile.write(\"#!\/bin\/bash\\n\\n\")\n\n    shellFile.write(\"cd base\/Project_Space\\n\\n\")\n\n    shellFile.write(\"# Download the SRA file\\n\")\n    shellFile.write(\"\/path\/to\/sra-toolkit\/bin\/prefetch -L debug -t fasp -v -v \" \\\n                     + SRA_ID + \"\\n\\n\")\n    shellFile.close()\n\ndef makeJobListFile():\n    jobListFile = open(\"base\/jobLists\/\" + batchName + \"_JobList.txt\", \"w\")\n    for i in SRA_list:\n        # Write the jobList for the Anisimov launcher\n        # Something like \"\/base\/batches\/batch1\/SRR123 SRR123.sh\"\n        jobListFile.write(batchFullPath + \"\/\" + i + \" \" + i + \".sh\\n\")\n    jobListFile.close()\n\n### IMPLEMENTATION\n\n# Get the list of SRA IDs\nwith open(sraListFile) as F:\n    for line in F:\n\tSRA_list.append(line.strip())\n\n# If the batch directory does not exist, create it\nif (not os.path.isdir(batchFullPath)):\n    os.mkdir(batchFullPath)\n# Create the subdirectories and shell scripts\nfor i in SRA_list:\n    createScripts(i)\n\nmakeJobListFile()\n<\/code><\/pre>\n<p>There are two arguments passed to this script at runtime: the location of the batchX.txt file created earlier and the name of the batch. It can be invoked with the following command:<\/p>\n<pre class=\"language-markup\"><code>python generateBatchScripts.prefetch.py \/base\/batches\/batch_lists\/batchX.txt batchX<\/code><\/pre>\n<p>This will generate the jobList.txt file and the necessary bash scripts and put them in their own directory within the batches folder:<\/p>\n<pre class=\"language-markup\"><code>\/base\/batches\/batchX\/SRR123\/SRR123.sh\n\/base\/batches\/batchX\/SRR234\/SRR234.sh\n\/base\/batches\/batchX\/SRR345\/SRR345.sh<\/code><\/pre>\n<p>... and so on.<\/p>\n<h2><a target=\"_blank\"><\/a>Step 3: Create the prefetch qsub script<\/h2>\n<p>To use the Anisimov Launcher to schedule jobs, construct a qsub script. These are stored in the \/base\/qsubs directory. Our tests suggest that 15 samples can be downloaded on a node simultaneously, as long as they are spaced out over the cores (on a cray system we supply the \u201c-d 2\u201d flag to aprun).<\/p>\n<pre class=\"language-markup\"><code>#nodes = batch_size\/15 \nAprun\u2019s -n flag = #nodes * 16\nAprun\u2019s -N flag = 16\n<\/code><\/pre>\n<p>The qsub script should look something like the example qsub below, which assumes a batch of 1,200 samples.<\/p>\n<pre class=\"language-markup\"><code>\/base\/qsubs\/batchX_prefetch.qsub \n\n#!\/bin\/bash\n#PBS -N sra_X\n#PBS -l walltime=1:15:00\n#PBS -l nodes=80:ppn=32\n#PBS -A groupid\n#PBS -q normal\n\naprun -n 1280 -N 16 -ss -d 2 ~anisimov\/scheduler\/scheduler.x \\   \n  \/base\/jobLists\/batchX_JobList.txt \/bin\/bash -noexit &amp;&gt; \/base\/qsubs\/logs\/batchX_prefetch.log\n<\/code><\/pre>\n<p>This script will launch all the bash scripts (15\/node). Both stdout and stderr will be piped to \/base\/qsubs\/logs\/batchX_prefetch.log.<\/p>\n<h2><a target=\"_blank\"><\/a>Step 4: Running the prefetch qsub script<\/h2>\n<p>Unfortunately, as prefetch runs, some of the SRA downloads will fail. To prevent a single failure from killing the Anisimov Job and the other downloads occurring in parallel, the -noexit flag is used (see the box above). However, this means the download may eventually reach a point at which all the SRAs have finished downloading, but the job just sits without making progress. This is just a consequence of the Anisimov Launcher code design.<\/p>\n<p>\u00a0<\/p>\n<p>To prevent this from wasting resources, monitor the size of the \/base\/Project_Space\/sra folder during the download using the following command:<\/p>\n<pre class=\"language-markup\"><code>ls \u2013l \/base\/Project_Space\/sra | head<\/code><\/pre>\n<p>If the size of the sra\/ folder does not appear to grow for five minutes or so, go ahead and kill the job (yes, it is a hack at this point, and one could automate it if desired):<\/p>\n<pre class=\"language-markup\"><code>qdel JobID<\/code><\/pre>\n<p>As the SRAs are downloaded, temporary files are generated in the sra\/ folder. If those files are present the next time you attempt to download this ID, the download will fail. To prevent this from happening, delete the .tmp and .lock files with the following commands:<\/p>\n<pre class=\"language-markup\"><code>cd \/base\/Project_Space\/sra\n\nrm *.tmp.aspera-ckpt\nrm *.tmp.partial\nrm *.lock\nrm *.tmp\n\nrm *.vdbcache.cache\nrm *.vdbcache\n<\/code><\/pre>\n<h2><a target=\"_blank\"><\/a>Step 5: Determine which SRA IDs did not finish downloading<\/h2>\n<p>After removing the .tmp and .lock files, run the following script:<\/p>\n<pre class=\"language-markup\"><code>\/bash\/scripts\/checkSRAsDownloaded.py\n\nimport sys\nimport glob\n\nbatch_list = sys.argv[1]\n\n# List of IDs\nbatch_IDs = []\n\nwith open(batch_list) as F:\n    for line in F:\n        batch_IDs.append(line.strip())\n\nIDs_found = []\n\nfor f in glob.glob(\"\/base\/Project_Space\/sra\/*\"):\n    split_string = f.split(\"\/\")\n    ID = split_string[-1].split('.')[0]\n    IDs_found.append(ID)\n\n# Remove redundant\nIDs_found = list(set(IDs_found))\n\ncount_missing = 0\n\nfor i in batch_IDs:\n    if i not in IDs_found:\n        print(i)\n        count_missing += 1\n\nprint(\"\\nIDs that are missing\")\nprint(count_missing)\n<\/code><\/pre>\n<p>This script grabs the IDs in the batch file and checks to see whether each SRA file is found in the sra\/ folder. Invoke with the following command:<\/p>\n<pre class=\"language-markup\"><code>python \/base\/scripts\/checkSRAsDownloaded.py \/base\/batches\/batch_lists\/batchX.txt<\/code><\/pre>\n<p>Any SRA IDs that are not present in the sra\/ folder will be printed out, as well as the total number that were not downloaded. Copy these IDs and put them in a new batch.txt file in \/base\/batches\/batch_lists. We found it effective to name this new file batchX.1, then name the next iteration batchX.2, and so on. Repeat the downloading steps until all SRA IDs are accounted for.<\/p>\n<p>\u00a0<\/p>\n<p><em>Note: On each iteration, reduce the resources requested in each qsub script so that resources are not wasted.<\/em><\/p>"},"is_project":0}]},{"id":"591103","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"A8DCD3922E954EAEA40C2FADD7560486","previous_guid":"6F0A5FAA05EF4DFBA6DD8000724224FF","previous_id":"591121","last_modified":"1508263474","components":[{"component_id":"1013127","previous_id":0,"original_id":"0","guid":"E1A7EEEBA2094ADCA0A28ABD83B0398D","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Acknowledgements","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Acknowledgements"},"is_project":0},{"component_id":"1013126","previous_id":"1013127","original_id":"0","guid":"FA903EC92DCF40EC8EA1D769F5CC86B4","previous_guid":"E1A7EEEBA2094ADCA0A28ABD83B0398D","component_type_id":"1","data_id":null,"data":"<p>We are grateful to Ms. Katherine Kendig for editorial help with this guide. This research is part of the Blue Waters sustained-petascale computing project, which is supported by the National Science Foundation (awards OCI-0725070 and ACI-1238993) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications.<\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>We are grateful to Ms. Katherine Kendig for editorial help with this guide. This research is part of the Blue Waters sustained-petascale computing project, which is supported by the National Science Foundation (awards OCI-0725070 and ACI-1238993) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications.<\/p>"},"is_project":0}]},{"id":"591121","is_changed":"0","original_id":"0","is_skipped":"0","is_checked":"0","guid":"6F0A5FAA05EF4DFBA6DD8000724224FF","previous_guid":"B964533356A34C5A896443562DB42400","previous_id":"591101","last_modified":"1508264463","components":[{"component_id":"1013182","previous_id":0,"original_id":"0","guid":"76DAACDA3F0D4850A1424111F6A7BCB4","previous_guid":null,"component_type_id":"6","data_id":"0","data":"Data Conversion: SRA to fastq.gz","order_id":"0","name":"Section","data_by_id":"0","type_id":"6","source_data":{"section":"Data Conversion: SRA to fastq.gz"},"is_project":0},{"component_id":"1013181","previous_id":"1013182","original_id":"0","guid":"86C8B522E8954A1F9052C864E6A183A2","previous_guid":"76DAACDA3F0D4850A1424111F6A7BCB4","component_type_id":"1","data_id":null,"data":"<p>Because the refseq reference data ares already downloaded, it will be easy to convert the SRA files to fastq.gz files.<\/p>\n<p>\u00a0<\/p>\n<p>However, first, it makes sense to check that the SRA files are intact using sra-validate. Call both<\/p>\n<p>sra-validate and fastq-dump in the same shell script, as shown in the following example script:<\/p>\n<pre class=\"language-markup\"><code>#!\/bin\/bash\n\ncd \/base\/Project_Space\n\n\/path\/to\/sra-toolkit\/bin\/vdb-validate &lt;SAMPLE_ID&gt;.sra &amp;&gt;   \\\n    \/base\/Project_Space\/validation_outputs\/batchX\/&lt;SAMPLE_ID&gt;.validation_out\n\nif grep -q 'err' \/base\/Project_Space\/validation_outputs\/batchX\/&lt;SAMPLE_ID&gt;.validation_out; then\n        echo 'Verification of &lt;SAMPLE_ID&gt;.sra failed'\n        cp \/base\/Project_Space\/validation_outputs\/batchX\/&lt;SAMPLE_ID&gt;.validation_out  \\\n           \/projects\/sciteam\/baib\/InputData_DoNotTouch\/dbGaP-13335\/validation_failures\/batch5\nelse\n        echo 'No errors found in &lt;SAMPLE_ID&gt;.sra'\n        # Convert the SRA into fastq\n        \/path\/to\/sra-toolkit\/bin\/fastq-dump -v --gzip --split-files \\ \n               -O \/base\/fastq_files\/batchX \/base\/Project_Space\/sra\/&lt;SAMPLE_ID&gt;.sra\nfi\n<\/code><\/pre>\n<p>This script will run sra-validate and store its output in the validation_out. If an error is found, the output is copied into the validation_error\/ folder and fastq-dump is not run. Otherwise, fastq-dump runs as expected.<\/p>\n<p>\u00a0<\/p>\n<p>These Anisimov launcher scripts are generated in the same way that the prefetch scripts were generated earlier, with a python script that generates the jobList.txt file and the shell scripts for each sample (next page).<\/p>\n<pre class=\"language-markup\"><code>\/base\/scripts\/generateBatchScripts.fastq-dump.py (modify with paths to suit your needs)\n\n#!\/usr\/bin\/python\n\nimport os\nimport os.path\nimport sys\n\n\n\n### GLOBAL VARIABLES\n\nsraListFile = sys.argv[1]\nbatchName = sys.argv[2]\n\nbatchFullPath = \"\/base\/batches\/\" + batchName\n\nSRA_list = []\n\n### FUNCTION DEFINITIONS\n\ndef createScripts(SRA_ID):\n    subDirName = batchFullPath + \"\/\" + SRA_ID\n\n    # Create the subdirectory within the batch directory\n    if (not os.path.isdir(subDirName)):\n        os.mkdir(subDirName)\n    \n    # Create the shell script file\n    shellFile = open(subDirName + \"\/\" + SRA_ID + \".sh\", \"w\")\n\n    # Write to the file\n    shellFile.write(\"#!\/bin\/bash\\n\\n\")\n\n    shellFile.write(\"cd \/base\/Project_Space\\n\\n\")\n\n    shellFile.write(\"\/path\/to\/sra-toolkit\/bin\/vdb-validate \" + SRA_ID + \".sra &amp;&gt; \\ \n                     \/base\/Project_Space\/validation_outputs\/\" + batchName + \"\/\" +  SRA_ID \\\n                     + \".validation_out\\n\\n\")\n\n    shellFile.write(\"if grep -q 'err' \/base\/Project_Space\/validation_outputs\/\" \\ \n                     + batchName + \"\/\" + SRA_ID + \".validation_out; then\\n\")\n    shellFile.write(\"\\techo 'Verification of \" + SRA_ID + \".sra failed'\\n\")\n    shellFile.write(\"\\tcp \/base\/Project_Space\/validation_outputs\/\" + batchName + \"\/\" \\\n                      + SRA_ID + \".validation_out \/base\/Project_Space\/validation_failures\/\" \\\n                      + batchName + \"\\n\")\n    shellFile.write(\"else\\n\")\n    shellFile.write(\"\\techo 'No errors found in \" + SRA_ID + \".sra'\\n\")\n    \n    shellFile.write(\"\\t# Convert the SRA into fastq\\n\")\n    shellFile.write(\"\\t\/path\/to\/sra-toolkit\/bin\/fastq-dump -v --gzip --split-files \\\n                      -O \/base\/fastq_files\/\" + batchName + \" \/base\/Project_Space\/sra\/\" \\\n                      + SRA_ID + \".sra\\n\")\n    shellFile.write(\"fi\\n\")\n\n    shellFile.close()\ndef makeJobListFile():\n    jobListFile = open(\"\/base\/jobLists\/\" + batchName + \"_JobList.txt\", \"w\")\n    for i in SRA_list:\n        # Write the jobList for the Anisimov launcher\n        # Something like \"\/base\/batches\/batch1\/SRR123 SRR123.sh\"\n        jobListFile.write(batchFullPath + \"\/\" + i + \" \" + i + \".sh\\n\")\n    jobListFile.close()\n\n\n### IMPLEMENTATION\n\n# Get the list of SRA IDs\nwith open(sraListFile) as F:\n    for line in F:\n\tSRA_list.append(line.strip())\n\n# If the batch directory does not exist, create it\nif (not os.path.isdir(batchFullPath)):\n    os.mkdir(batchFullPath)\n\n\n\n# Create the subdirectories and shell scripts\nfor i in SRA_list:\n    createScripts(i)\n\nmakeJobListFile()\n\ntry:# Create directory in the validation folders\n    os.mkdir(\"\/base\/Project_Space\/validation_outputs\/\" + batchName)\n    os.mkdir(\u201c\/base\/Project_Space\/validation_failures\/\" + batchName)\nexcept:\n    pass\n<\/code><\/pre>\n<p>\u00a0<\/p>\n<p>This script is invoked in the same way that the prefetch script generator was:<\/p>\n<pre class=\"language-markup\"><code>cd \/base\/scripts\npython generateBatchScripts.fastq-dump.py ..\/batches\/batch_lists\/batchX.txt batchX\n<\/code><\/pre>\n<p>\u00a0<\/p>\n<p><em>Note: This script will overwrite the jobList and the batch scripts generated from the prefetch generator for batchX. However, all the files will have already been downloaded.<\/em><\/p>\n<h2><a target=\"_blank\"><\/a>Create and run the fastq-dump qsub script<\/h2>\n<p>The SRA to FASTQ conversion itself typically proceeds without error. Although the batch was downloaded in iterations, the whole batch can generally be converted in one step.<\/p>\n<p>\u00a0<\/p>\n<p>The qsub looks something like the following, assuming a batch size of 1,200 samples:<\/p>\n<pre class=\"language-markup\"><code>\/base\/qsubs\/batchX_fastq-dump.qsub \n\n#!\/bin\/bash\n#PBS -N sra_X\n#PBS -l walltime=6:00:00\n#PBS -l nodes=80:ppn=32\n#PBS -A groupid\n#PBS -q normal\n\naprun -n 1280 -N 16 -ss -d 2 ~anisimov\/scheduler\/scheduler.x \/base\/jobLists\/batchX_JobList.txt \/bin\/bash -noexit &amp;&gt; \/base\/qsubs\/logs\/batchX_fastq-dump.log\n<\/code><\/pre>\n<p>This script will launch all the bash scripts (15\/node). Both stdout and stderr will be piped to \/base\/qsubs\/logs\/batchX_fastq-dump.log.<\/p>\n<p>\u00a0<\/p>\n<p>After this script completes, run the following script to verify that all the fastq files are present:<\/p>\n<pre class=\"language-markup\"><code>\/base\/scripts\/checkFastqsConverted.py\n\n#!\/usr\/bin python\n\"\"\"\nThis script checks to see how many IDs in a given list are found within the sra folder\nIt prints those that are not present\n\n\"\"\"\nimport sys\nimport glob\n\nbatch_list = sys.argv[1]\nfastq_batch = sys.argv[2]\n\n# List of IDs\nbatch_IDs = []\n\nwith open(batch_list) as F:\n    for line in F:\n        batch_IDs.append(line.strip())\n\nfastqs_found = []\n\nfor f in glob.glob(fastq_batch + \"\/*\"):\n    split_line = f.split(\"\/\")\n    fastqs_found.append(split_line[-1])\n\ncount_missing = 0\n\n\n\nfor i in batch_IDs:\n    fq1 = i + \"_1.fastq.gz\"\n    fq2 = i + \"_2.fastq.gz\"\n    if fq1 not in fastqs_found:\n        print fq1\n        count_missing += 1\n    else:\n\t# Remove it from the list, so if any IDs are left in the end, those IDs should not be in this directory\n\tfastqs_found.remove(fq1)\n    if fq2 not in fastqs_found:\n        print fq2\n        count_missing += 1\n    else:\n\tfastqs_found.remove(fq2)\n\nprint(\"\\nFastq files that are missing\")\nprint(count_missing)\n\nprint(\"\\nThese IDs were found but shouldn't be here\")\nprint(fastqs_found)\n<\/code><\/pre>\n<p>Invoke with the following command:<\/p>\n<pre class=\"language-markup\"><code>python \/base\/scripts\/checkFastqsConverted.py \/base\/batches\/batch_lists\/batchX.txt \/base\/fastq_files\/batchX\n<\/code><\/pre>\n<p>At this point, the download and conversion are complete. If any fastq files are absent, inspect the validation_error files to find out why. Re-download the SRA files if necessary.<\/p>\n<p><strong>\u00a0<\/strong><\/p>\n<p><strong>You now have a complete set of FASTQ files from NCBI. We hope you found this protocol useful.<\/strong><\/p>","order_id":"1","name":"Description","data_by_id":"0","type_id":"1","source_data":{"description":"<p>Because the refseq reference data ares already downloaded, it will be easy to convert the SRA files to fastq.gz files.<\/p>\n<p>\u00a0<\/p>\n<p>However, first, it makes sense to check that the SRA files are intact using sra-validate. Call both<\/p>\n<p>sra-validate and fastq-dump in the same shell script, as shown in the following example script:<\/p>\n<pre class=\"language-markup\"><code>#!\/bin\/bash\n\ncd \/base\/Project_Space\n\n\/path\/to\/sra-toolkit\/bin\/vdb-validate &lt;SAMPLE_ID&gt;.sra &amp;&gt;   \\\n    \/base\/Project_Space\/validation_outputs\/batchX\/&lt;SAMPLE_ID&gt;.validation_out\n\nif grep -q 'err' \/base\/Project_Space\/validation_outputs\/batchX\/&lt;SAMPLE_ID&gt;.validation_out; then\n        echo 'Verification of &lt;SAMPLE_ID&gt;.sra failed'\n        cp \/base\/Project_Space\/validation_outputs\/batchX\/&lt;SAMPLE_ID&gt;.validation_out  \\\n           \/projects\/sciteam\/baib\/InputData_DoNotTouch\/dbGaP-13335\/validation_failures\/batch5\nelse\n        echo 'No errors found in &lt;SAMPLE_ID&gt;.sra'\n        # Convert the SRA into fastq\n        \/path\/to\/sra-toolkit\/bin\/fastq-dump -v --gzip --split-files \\ \n               -O \/base\/fastq_files\/batchX \/base\/Project_Space\/sra\/&lt;SAMPLE_ID&gt;.sra\nfi\n<\/code><\/pre>\n<p>This script will run sra-validate and store its output in the validation_out. If an error is found, the output is copied into the validation_error\/ folder and fastq-dump is not run. Otherwise, fastq-dump runs as expected.<\/p>\n<p>\u00a0<\/p>\n<p>These Anisimov launcher scripts are generated in the same way that the prefetch scripts were generated earlier, with a python script that generates the jobList.txt file and the shell scripts for each sample (next page).<\/p>\n<pre class=\"language-markup\"><code>\/base\/scripts\/generateBatchScripts.fastq-dump.py (modify with paths to suit your needs)\n\n#!\/usr\/bin\/python\n\nimport os\nimport os.path\nimport sys\n\n\n\n### GLOBAL VARIABLES\n\nsraListFile = sys.argv[1]\nbatchName = sys.argv[2]\n\nbatchFullPath = \"\/base\/batches\/\" + batchName\n\nSRA_list = []\n\n### FUNCTION DEFINITIONS\n\ndef createScripts(SRA_ID):\n    subDirName = batchFullPath + \"\/\" + SRA_ID\n\n    # Create the subdirectory within the batch directory\n    if (not os.path.isdir(subDirName)):\n        os.mkdir(subDirName)\n    \n    # Create the shell script file\n    shellFile = open(subDirName + \"\/\" + SRA_ID + \".sh\", \"w\")\n\n    # Write to the file\n    shellFile.write(\"#!\/bin\/bash\\n\\n\")\n\n    shellFile.write(\"cd \/base\/Project_Space\\n\\n\")\n\n    shellFile.write(\"\/path\/to\/sra-toolkit\/bin\/vdb-validate \" + SRA_ID + \".sra &amp;&gt; \\ \n                     \/base\/Project_Space\/validation_outputs\/\" + batchName + \"\/\" +  SRA_ID \\\n                     + \".validation_out\\n\\n\")\n\n    shellFile.write(\"if grep -q 'err' \/base\/Project_Space\/validation_outputs\/\" \\ \n                     + batchName + \"\/\" + SRA_ID + \".validation_out; then\\n\")\n    shellFile.write(\"\\techo 'Verification of \" + SRA_ID + \".sra failed'\\n\")\n    shellFile.write(\"\\tcp \/base\/Project_Space\/validation_outputs\/\" + batchName + \"\/\" \\\n                      + SRA_ID + \".validation_out \/base\/Project_Space\/validation_failures\/\" \\\n                      + batchName + \"\\n\")\n    shellFile.write(\"else\\n\")\n    shellFile.write(\"\\techo 'No errors found in \" + SRA_ID + \".sra'\\n\")\n    \n    shellFile.write(\"\\t# Convert the SRA into fastq\\n\")\n    shellFile.write(\"\\t\/path\/to\/sra-toolkit\/bin\/fastq-dump -v --gzip --split-files \\\n                      -O \/base\/fastq_files\/\" + batchName + \" \/base\/Project_Space\/sra\/\" \\\n                      + SRA_ID + \".sra\\n\")\n    shellFile.write(\"fi\\n\")\n\n    shellFile.close()\ndef makeJobListFile():\n    jobListFile = open(\"\/base\/jobLists\/\" + batchName + \"_JobList.txt\", \"w\")\n    for i in SRA_list:\n        # Write the jobList for the Anisimov launcher\n        # Something like \"\/base\/batches\/batch1\/SRR123 SRR123.sh\"\n        jobListFile.write(batchFullPath + \"\/\" + i + \" \" + i + \".sh\\n\")\n    jobListFile.close()\n\n\n### IMPLEMENTATION\n\n# Get the list of SRA IDs\nwith open(sraListFile) as F:\n    for line in F:\n\tSRA_list.append(line.strip())\n\n# If the batch directory does not exist, create it\nif (not os.path.isdir(batchFullPath)):\n    os.mkdir(batchFullPath)\n\n\n\n# Create the subdirectories and shell scripts\nfor i in SRA_list:\n    createScripts(i)\n\nmakeJobListFile()\n\ntry:# Create directory in the validation folders\n    os.mkdir(\"\/base\/Project_Space\/validation_outputs\/\" + batchName)\n    os.mkdir(\u201c\/base\/Project_Space\/validation_failures\/\" + batchName)\nexcept:\n    pass\n<\/code><\/pre>\n<p>\u00a0<\/p>\n<p>This script is invoked in the same way that the prefetch script generator was:<\/p>\n<pre class=\"language-markup\"><code>cd \/base\/scripts\npython generateBatchScripts.fastq-dump.py ..\/batches\/batch_lists\/batchX.txt batchX\n<\/code><\/pre>\n<p>\u00a0<\/p>\n<p><em>Note: This script will overwrite the jobList and the batch scripts generated from the prefetch generator for batchX. However, all the files will have already been downloaded.<\/em><\/p>\n<h2><a target=\"_blank\"><\/a>Create and run the fastq-dump qsub script<\/h2>\n<p>The SRA to FASTQ conversion itself typically proceeds without error. Although the batch was downloaded in iterations, the whole batch can generally be converted in one step.<\/p>\n<p>\u00a0<\/p>\n<p>The qsub looks something like the following, assuming a batch size of 1,200 samples:<\/p>\n<pre class=\"language-markup\"><code>\/base\/qsubs\/batchX_fastq-dump.qsub \n\n#!\/bin\/bash\n#PBS -N sra_X\n#PBS -l walltime=6:00:00\n#PBS -l nodes=80:ppn=32\n#PBS -A groupid\n#PBS -q normal\n\naprun -n 1280 -N 16 -ss -d 2 ~anisimov\/scheduler\/scheduler.x \/base\/jobLists\/batchX_JobList.txt \/bin\/bash -noexit &amp;&gt; \/base\/qsubs\/logs\/batchX_fastq-dump.log\n<\/code><\/pre>\n<p>This script will launch all the bash scripts (15\/node). Both stdout and stderr will be piped to \/base\/qsubs\/logs\/batchX_fastq-dump.log.<\/p>\n<p>\u00a0<\/p>\n<p>After this script completes, run the following script to verify that all the fastq files are present:<\/p>\n<pre class=\"language-markup\"><code>\/base\/scripts\/checkFastqsConverted.py\n\n#!\/usr\/bin python\n\"\"\"\nThis script checks to see how many IDs in a given list are found within the sra folder\nIt prints those that are not present\n\n\"\"\"\nimport sys\nimport glob\n\nbatch_list = sys.argv[1]\nfastq_batch = sys.argv[2]\n\n# List of IDs\nbatch_IDs = []\n\nwith open(batch_list) as F:\n    for line in F:\n        batch_IDs.append(line.strip())\n\nfastqs_found = []\n\nfor f in glob.glob(fastq_batch + \"\/*\"):\n    split_line = f.split(\"\/\")\n    fastqs_found.append(split_line[-1])\n\ncount_missing = 0\n\n\n\nfor i in batch_IDs:\n    fq1 = i + \"_1.fastq.gz\"\n    fq2 = i + \"_2.fastq.gz\"\n    if fq1 not in fastqs_found:\n        print fq1\n        count_missing += 1\n    else:\n\t# Remove it from the list, so if any IDs are left in the end, those IDs should not be in this directory\n\tfastqs_found.remove(fq1)\n    if fq2 not in fastqs_found:\n        print fq2\n        count_missing += 1\n    else:\n\tfastqs_found.remove(fq2)\n\nprint(\"\\nFastq files that are missing\")\nprint(count_missing)\n\nprint(\"\\nThese IDs were found but shouldn't be here\")\nprint(fastqs_found)\n<\/code><\/pre>\n<p>Invoke with the following command:<\/p>\n<pre class=\"language-markup\"><code>python \/base\/scripts\/checkFastqsConverted.py \/base\/batches\/batch_lists\/batchX.txt \/base\/fastq_files\/batchX\n<\/code><\/pre>\n<p>At this point, the download and conversion are complete. If any fastq files are absent, inspect the validation_error files to find out why. Re-download the SRA files if necessary.<\/p>\n<p><strong>\u00a0<\/strong><\/p>\n<p><strong>You now have a complete set of FASTQ files from NCBI. We hope you found this protocol useful.<\/strong><\/p>"},"is_project":0}]}]}